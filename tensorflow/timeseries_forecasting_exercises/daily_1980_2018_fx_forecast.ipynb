{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "daily_1980_2018_fx_forecast.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9RS9W8EnMC9t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Resources and references\n",
        "And the data are gethared from *https://investing.com*\n",
        "\n",
        "# Data Info\n",
        "In our data, the column names were not given but the order is CLOSE, OPEN, HIGH, LOW and CHANGE\n",
        "\n",
        "Data type: EUR/USD daily data.\n",
        "\n",
        "the dates between 01/01/1980 and 28/02/2019 were used for train, validation.\n",
        "\n",
        "test data would be march april or even other future dates.\n",
        "\n",
        "**important: data should be reversed after read with pandas. The dataset from the investing.com is reversed. This effects prediction.**\n",
        "\n",
        "# Task Info\n",
        "\n",
        "Binary classification.\n",
        "Classifies if the given time will have a higher value or lower value.\n",
        "\n",
        "Later: Regression\n",
        "Tries to predict exact price.\n",
        "\n",
        "# Purpose\n",
        "\n",
        "- find out if using more data on daily forecasting is a good idea\n",
        "- use regression to predict a value"
      ]
    },
    {
      "metadata": {
        "id": "oSrym8CVMWil",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports and Mount google drive"
      ]
    },
    {
      "metadata": {
        "id": "99ZwGfyMLTgw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, absolute_import, division\n",
        "\n",
        "# general imports for deep learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# data read\n",
        "import pandas as pd\n",
        "\n",
        "# plot\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# json and pretty print\n",
        "import json\n",
        "import pprint\n",
        "\n",
        "# to persist the numpy arrays data\n",
        "import h5py\n",
        "\n",
        "# handle logging\n",
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iLEQJOmPMbto",
        "colab_type": "code",
        "outputId": "891ede3a-8327-4954-9cda-5ccd7e189aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tK_NZWs2MdMd",
        "colab_type": "code",
        "outputId": "e4dbd06f-ef8f-4e94-db3a-1b21dbce6d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# check if correct place\n",
        "!ls '/content/gdrive/My Drive/deep_learning/_data/forex/daily_csv/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " EUR_USD_1980_1990.csv\t EUR_USD_2019_jan_feb.csv\n",
            " EUR_USD_1991_2000.csv\t'EUR_USD Historical Data.csv'\n",
            " EUR_USD_2001_2018.csv\t'EUR_USD Historical Data.gsheet'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6DMRTRlNM0rZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading Data"
      ]
    },
    {
      "metadata": {
        "id": "tu33ziOdNlrA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Functions of Data Handling"
      ]
    },
    {
      "metadata": {
        "id": "LYFiTGIFNqSV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Tries to concatenate a list of arrays into one array\n",
        "def get_concatenated_dataset(d_list):\n",
        "  result_data = d_list[0]\n",
        "  for d in d_list[1:]:\n",
        "    result_data = np.concatenate((result_data, d), axis=None)\n",
        "    \n",
        "  return result_data\n",
        "\n",
        "# Tries to check if the concatenated list is correct.\n",
        "def concatenate_length_check(d_list, concatenated):\n",
        "  print(\"----------- length check -----------\")\n",
        "  total_length = 0\n",
        "  for d in d_list:  \n",
        "    total_length += len(d)\n",
        "    print(\"length: \" +str(len(d)))\n",
        "\n",
        "  print(\"concatenated length \"+str(len(concatenated)))\n",
        "  if(len(concatenated) == total_length):\n",
        "    print(\"concatenated length -----------> CORRECT\")\n",
        "  else:\n",
        "    print(\"concatenated length -----------> WRONG\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dSvq8s2mNh7Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read Data"
      ]
    },
    {
      "metadata": {
        "id": "Fm9MAQUXMfWY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initialize file names\n",
        "data_folder = \"/content/gdrive/My Drive/deep_learning/_data/forex/daily_csv/\"\n",
        "data_filenames = []\n",
        "data_filenames.append(\"EUR_USD_2001_2018.csv\")\n",
        "data_filenames.append(\"EUR_USD_1991_2000.csv\")\n",
        "data_filenames.append(\"EUR_USD_1980_1990.csv\")\n",
        "\n",
        "test_data_filenames = []\n",
        "test_data_filenames.append(\"EUR_USD_2019_jan_feb.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T6ndJ1BbNOFx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get train data that will be both validation and train data in training mode\n",
        "data_1 = pd.read_csv(data_folder+data_filenames[0])\n",
        "data_2 = pd.read_csv(data_folder+data_filenames[1])\n",
        "data_3 = pd.read_csv(data_folder+data_filenames[2])\n",
        "\n",
        "# Get all data as list\n",
        "data_list = [data_1, data_2, data_3]\n",
        "\n",
        "test_data_list = [pd.read_csv(data_folder+test_data_filenames[0])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TPWmtfaLNXC8",
        "colab_type": "code",
        "outputId": "18edb5b6-4a3d-42a6-82eb-8f278337da46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# print for understand the context\n",
        "# NOTE: it is reversed\n",
        "data_3.tail()\n",
        "#test_data_list[0].head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Price</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Change %</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2806</th>\n",
              "      <td>Jan 08, 1980</td>\n",
              "      <td>1.5108</td>\n",
              "      <td>1.5108</td>\n",
              "      <td>1.5108</td>\n",
              "      <td>1.5108</td>\n",
              "      <td>-0.40%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2807</th>\n",
              "      <td>Jan 07, 1980</td>\n",
              "      <td>1.5168</td>\n",
              "      <td>1.5168</td>\n",
              "      <td>1.5168</td>\n",
              "      <td>1.5168</td>\n",
              "      <td>0.26%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2808</th>\n",
              "      <td>Jan 04, 1980</td>\n",
              "      <td>1.5129</td>\n",
              "      <td>1.5129</td>\n",
              "      <td>1.5129</td>\n",
              "      <td>1.5129</td>\n",
              "      <td>-0.32%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2809</th>\n",
              "      <td>Jan 03, 1980</td>\n",
              "      <td>1.5177</td>\n",
              "      <td>1.5177</td>\n",
              "      <td>1.5177</td>\n",
              "      <td>1.5177</td>\n",
              "      <td>0.18%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2810</th>\n",
              "      <td>Jan 02, 1980</td>\n",
              "      <td>1.5149</td>\n",
              "      <td>1.5149</td>\n",
              "      <td>1.5149</td>\n",
              "      <td>1.5149</td>\n",
              "      <td>0.45%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Date   Price    Open    High     Low Change %\n",
              "2806  Jan 08, 1980  1.5108  1.5108  1.5108  1.5108   -0.40%\n",
              "2807  Jan 07, 1980  1.5168  1.5168  1.5168  1.5168    0.26%\n",
              "2808  Jan 04, 1980  1.5129  1.5129  1.5129  1.5129   -0.32%\n",
              "2809  Jan 03, 1980  1.5177  1.5177  1.5177  1.5177    0.18%\n",
              "2810  Jan 02, 1980  1.5149  1.5149  1.5149  1.5149    0.45%"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "dHSQqBv8SBqr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_P59SlJoNjXc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read Train Data"
      ]
    },
    {
      "metadata": {
        "id": "E1JvKgE4OBvu",
        "colab_type": "code",
        "outputId": "70e08ab5-4b6e-4f35-86e6-b91b77b1b51b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "# Get OPEN, HIGH, LOW, CLOSE columns in all data\n",
        "open_data_list=[]\n",
        "high_data_list=[]\n",
        "low_data_list=[]\n",
        "close_data_list=[]\n",
        "\n",
        "for d in data_list:\n",
        "  open_data_list.append(d['Open'].as_matrix())\n",
        "  high_data_list.append(d['High'].as_matrix())\n",
        "  low_data_list.append(d['Low'].as_matrix())\n",
        "  close_data_list.append(d['Price'].as_matrix())\n",
        "  \n",
        "# And CONCATENATE all of them\n",
        "all_open_data = get_concatenated_dataset(open_data_list)\n",
        "all_high_data = get_concatenated_dataset(high_data_list)\n",
        "all_low_data = get_concatenated_dataset(low_data_list)\n",
        "all_close_data = get_concatenated_dataset(close_data_list)\n",
        "  \n",
        "# CHECK IF CONCATENATION IS SUCCESSUL.\n",
        "concatenate_length_check(open_data_list, all_open_data)\n",
        "concatenate_length_check(high_data_list, all_high_data)\n",
        "concatenate_length_check(low_data_list, all_low_data)\n",
        "concatenate_length_check(close_data_list, all_close_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------- length check -----------\n",
            "length: 4696\n",
            "length: 2603\n",
            "length: 2811\n",
            "concatenated length 10110\n",
            "concatenated length -----------> CORRECT\n",
            "----------- length check -----------\n",
            "length: 4696\n",
            "length: 2603\n",
            "length: 2811\n",
            "concatenated length 10110\n",
            "concatenated length -----------> CORRECT\n",
            "----------- length check -----------\n",
            "length: 4696\n",
            "length: 2603\n",
            "length: 2811\n",
            "concatenated length 10110\n",
            "concatenated length -----------> CORRECT\n",
            "----------- length check -----------\n",
            "length: 4696\n",
            "length: 2603\n",
            "length: 2811\n",
            "concatenated length 10110\n",
            "concatenated length -----------> CORRECT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9Mazu3DDOEp4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read Test Data"
      ]
    },
    {
      "metadata": {
        "id": "_AkeDXLLOGJv",
        "colab_type": "code",
        "outputId": "fffbc7c0-0787-4724-87bb-e5a38452ee21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "# Get OPEN, HIGH, LOW, CLOSE columns in all data\n",
        "t_open_data_list=[]\n",
        "t_high_data_list=[]\n",
        "t_low_data_list=[]\n",
        "t_close_data_list=[]\n",
        "\n",
        "for d in test_data_list:\n",
        "  t_open_data_list.append(d['Open'].as_matrix())\n",
        "  t_high_data_list.append(d['High'].as_matrix())\n",
        "  t_low_data_list.append(d['Low'].as_matrix())\n",
        "  t_close_data_list.append(d['Price'].as_matrix())\n",
        "  \n",
        "# And CONCATENATE all of them\n",
        "t_all_open_data = get_concatenated_dataset(t_open_data_list)\n",
        "t_all_high_data = get_concatenated_dataset(t_high_data_list)\n",
        "t_all_low_data = get_concatenated_dataset(t_low_data_list)\n",
        "t_all_close_data = get_concatenated_dataset(t_close_data_list)\n",
        "  \n",
        "# CHECK IF CONCATENATION IS SUCCESSUL.\n",
        "concatenate_length_check(t_open_data_list, t_all_open_data)\n",
        "concatenate_length_check(t_high_data_list, t_all_high_data)\n",
        "concatenate_length_check(t_low_data_list, t_all_low_data)\n",
        "concatenate_length_check(t_close_data_list, t_all_close_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------- length check -----------\n",
            "length: 63\n",
            "concatenated length 63\n",
            "concatenated length -----------> CORRECT\n",
            "----------- length check -----------\n",
            "length: 63\n",
            "concatenated length 63\n",
            "concatenated length -----------> CORRECT\n",
            "----------- length check -----------\n",
            "length: 63\n",
            "concatenated length 63\n",
            "concatenated length -----------> CORRECT\n",
            "----------- length check -----------\n",
            "length: 63\n",
            "concatenated length 63\n",
            "concatenated length -----------> CORRECT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3icqZHtxPlDJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Flip Data"
      ]
    },
    {
      "metadata": {
        "id": "zGTU3wx0PkVh",
        "colab_type": "code",
        "outputId": "9b6ff623-9dde-48da-e2fe-929ff7964999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# REVERSE THE DATA BEFORE CONTINUE\n",
        "all_open_data = np.flip(all_open_data, 0)\n",
        "all_high_data = np.flip(all_high_data, 0)\n",
        "all_low_data = np.flip(all_low_data, 0)\n",
        "all_close_data = np.flip(all_close_data, 0)\n",
        "\n",
        "t_all_open_data = np.flip(t_all_open_data, 0)\n",
        "t_all_high_data = np.flip(t_all_high_data, 0)\n",
        "t_all_low_data = np.flip(t_all_low_data, 0)\n",
        "t_all_close_data = np.flip(t_all_close_data, 0)\n",
        "\n",
        "print(\"OPEN HIGH LOW CLOSE data were flipped successfully for both rain and test data.\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OPEN HIGH LOW CLOSE data were flipped successfully for both rain and test data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eEp3S7tnR6wg",
        "colab_type": "code",
        "outputId": "ec1f0fc3-ce50-43b9-dd2a-2bee250bd06b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "all_open_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.5149, 1.5177, 1.5129, ..., 1.1353, 1.1429, 1.1443])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "bAnF6Rn4QwLv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# General Values"
      ]
    },
    {
      "metadata": {
        "id": "WJoAdvHdQ093",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# BATCH size of the lstm model\n",
        "#BATCH_SIZES = [32, 64, 128, 256, 512]\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# How many of the past points were involved.\n",
        "#WINDOWS = [32, 64, 128, 256, 512]\n",
        "WINDOW = 32\n",
        "\n",
        "# How many of data type is used as multivariate(open,high,low,close = 4)\n",
        "EMB_SIZE = 4\n",
        "\n",
        "# While training how many points should be ignored\n",
        "STEP = 1\n",
        "\n",
        "# Determines which time should be predictied\n",
        "#(1 = 1 min further is predicted)\n",
        "#(60 = 1 hour further is predicted)\n",
        "FORECAST = 1\n",
        "\n",
        "# Determines if the data is to be load.\n",
        "LOAD = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dLuSc2mhP6A0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Construct Data"
      ]
    },
    {
      "metadata": {
        "id": "RP7ZAQHlP7d1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ]
    },
    {
      "metadata": {
        "id": "9jCPvXIoQA2B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ORIGINAL CHANEGD TO REGRESSION\n",
        "def get_data_chunks(d_list, length, window=30, forecast=1, step=1):\n",
        "  X = []\n",
        "  Y = []\n",
        "  for i in range(0, length, step):\n",
        "    try:\n",
        "      # Get windowed data\n",
        "      o = d_list[0][i:i+window] # open\n",
        "      h = d_list[1][i:i+window] # high\n",
        "      l = d_list[2][i:i+window] # low\n",
        "      c = d_list[3][i:i+window] # close\n",
        "\n",
        "      # Normalize data\n",
        "      # NO NORMALIZATION FOR NOW\n",
        "      #o = (np.array(o) - np.mean(o)) / np.std(o)\n",
        "      #h = (np.array(h) - np.mean(h)) / np.std(h)\n",
        "      #l = (np.array(l) - np.mean(l)) / np.std(l)\n",
        "      #c = (np.array(c) - np.mean(c)) / np.std(c)\n",
        "\n",
        "      # x_i\n",
        "      x_i = d_list[3][i:i+window]\n",
        "      y_i = d_list[3][i+window+forecast]\n",
        "\n",
        "      x_i = np.column_stack((o,h,l,c))\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      # break when the limit is not enough\n",
        "      break\n",
        "\n",
        "    X.append(x_i)\n",
        "    Y.append(y_i)\n",
        "  print(\"data chunks are ready...\")\n",
        "  return [X, Y]\n",
        "\n",
        "def get_train_validation(X, y, percentage=0.8):\n",
        "    iXPercentage = int(len(X) * percentage) \n",
        "    iYPercentage = int(len(y) * percentage)\n",
        "    X_train = X[0:iXPercentage]\n",
        "    Y_train = y[0:iYPercentage]\n",
        "    \n",
        "    #X_train, Y_train = shuffle_in_unison(X_train, Y_train)\n",
        "\n",
        "    X_val = X[iXPercentage:]\n",
        "    Y_val = y[iYPercentage:]\n",
        "\n",
        "    return X_train, X_val, Y_train, Y_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gpd7g7uVQV_7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_data(x,y,x_test,y_test):\n",
        "  # Save X, Y and X_test, Y_test\n",
        "  save_folder = \"/content/gdrive/My Drive/deep_learning/_data/numpy_arrays/\"\n",
        "  filename=\"d_fx_multi_fcst_1980_2018\"\n",
        "\n",
        "  # Save Y\n",
        "  y_h5 = h5py.File(save_folder+filename+\"_y.h5\", 'w')\n",
        "  y_h5.create_dataset('dataset_Y', data=y)\n",
        "  y_h5.close()\n",
        "  print(\"Saving Y Completed\")\n",
        "\n",
        "  # Save X\n",
        "  x_h5 = h5py.File(save_folder+filename+\"_x.h5\", 'w')\n",
        "  x_h5.create_dataset('dataset_X', data=x)\n",
        "  x_h5.close()\n",
        "  print(\"Saving X Completed\")\n",
        "\n",
        "  # Save X_test\n",
        "  x_test_h5 = h5py.File(save_folder+filename+\"_x_test.h5\", 'w')\n",
        "  x_test_h5.create_dataset('dataset_X_test', data=x_test)\n",
        "  x_test_h5.close()\n",
        "  print(\"Saving X_test Completed\")\n",
        "\n",
        "  # Save Y_test\n",
        "  y_test_h5 = h5py.File(save_folder+filename+\"_y_test.h5\", 'w')\n",
        "  y_test_h5.create_dataset('dataset_Y_test', data=y_test)\n",
        "  y_test_h5.close()\n",
        "  print(\"Saving Y_test Completed\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YurLF_NaQgk7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(should_loaded, d_list, d_list_test, train_len, test_len, window):\n",
        "  if not should_loaded:\n",
        "    # PROCESSES WHOLE TRAIN SET\n",
        "    X, Y = get_data_chunks(d_list, train_len, window=window, forecast=FORECAST, step=STEP)\n",
        "    X_test, Y_test = get_data_chunks(d_list_test, test_len, window=window, forecast=FORECAST, step=STEP)\n",
        "\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "    X_test = np.array(X_test)\n",
        "    Y_test = np.array(Y_test)\n",
        "  else:\n",
        "    save_folder = \"/content/gdrive/My Drive/deep_learning/_data/numpy_arrays/\"\n",
        "    filename=\"d_fx_multi_fcst_1980_2018\"\n",
        "    \n",
        "    h5f = h5py.File(save_folder+filename+\"_x.h5\",'r')\n",
        "    X = h5f['dataset_X'][:]\n",
        "    h5f.close()\n",
        "\n",
        "    h5f = h5py.File(save_folder+filename+\"_y.h5\",'r')\n",
        "    Y = h5f['dataset_Y'][:]\n",
        "    h5f.close()\n",
        "\n",
        "    h5f = h5py.File(save_folder+filename+\"_x_test.h5\",'r')\n",
        "    X_test = h5f['dataset_X_test'][:]\n",
        "    h5f.close()\n",
        "\n",
        "    h5f = h5py.File(save_folder+filename+\"_y_test.h5\",'r')\n",
        "    Y_test = h5f['dataset_Y_test'][:]\n",
        "    h5f.close()\n",
        "    pass\n",
        "  \n",
        "  return [X, Y, X_test, Y_test]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J7urATEYR2A6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Get Data"
      ]
    },
    {
      "metadata": {
        "id": "56ikNBOYSjZM",
        "colab_type": "code",
        "outputId": "359a347e-f783-4c04-f615-a871e8ae4f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "X_list = []\n",
        "Y_list = []\n",
        "X_test_list = []\n",
        "Y_test_list = []\n",
        "if not LOAD:\n",
        "  d_list = [all_open_data, all_high_data, all_low_data, all_close_data]\n",
        "  d_list_test = [t_all_open_data, t_all_high_data, t_all_low_data, t_all_close_data]\n",
        "\n",
        "\n",
        "  for w in [WINDOW]:\n",
        "    print(\"getting data for window: \"+str(w))\n",
        "    X, Y, X_test, Y_test = get_data(should_loaded=LOAD, \n",
        "                                    d_list=d_list, \n",
        "                                    d_list_test=d_list_test, \n",
        "                                    train_len=len(all_close_data), \n",
        "                                    test_len=len(all_close_data), \n",
        "                                    window=w)\n",
        "    X_list.append(X)\n",
        "    Y_list.append(Y)\n",
        "    X_test_list.append(X_test)\n",
        "    Y_test_list.append(Y_test)\n",
        "else:\n",
        "    X, Y, X_test, Y_test = get_data(should_loaded=LOAD, \n",
        "                                d_list=None, \n",
        "                                d_list_test=None, \n",
        "                                train_len=0, \n",
        "                                test_len=0)\n",
        "#if not LOAD: save_data(X, Y, X_test, Y_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "getting data for window: 32\n",
            "index 10110 is out of bounds for axis 0 with size 10110\n",
            "data chunks are ready...\n",
            "index 63 is out of bounds for axis 0 with size 63\n",
            "data chunks are ready...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Afqz0A-wTu6e",
        "colab_type": "code",
        "outputId": "2300f78e-a476-4c41-f39d-ecf46731371f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(X_list[0].shape)\n",
        "print(X_test_list[0].shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10077, 32, 4)\n",
            "(30, 32, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eJPHPibfUGX6",
        "colab_type": "code",
        "outputId": "dd1c15fa-9552-4981-f9e1-377108239fc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "X_train_list = []\n",
        "X_val_list = []\n",
        "Y_train_list = []\n",
        "Y_val_list = []\n",
        "for i in range(len(X_list)):\n",
        "  X_train, X_val, Y_train, Y_val = get_train_validation(X_list[i], Y_list[i])\n",
        "  X_train_list.append(X_train)\n",
        "  X_val_list.append(X_val)\n",
        "  Y_train_list.append(Y_train)\n",
        "  Y_val_list.append(Y_val)\n",
        "  \n",
        "  print(\"OLD_shapes for windows: \"+str(WINDOW))\n",
        "  print(X_list[i].shape)\n",
        "  print(Y_list[i].shape)\n",
        "  print(\"*\"*40)\n",
        "  print(\"New Shapes for windows: \"+str(WINDOW))\n",
        "  print(X_train.shape)\n",
        "  print(Y_train.shape)\n",
        "  print(X_val.shape)\n",
        "  print(Y_val.shape)\n",
        "  print(\"-\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OLD_shapes for windows: 32\n",
            "(10077, 32, 4)\n",
            "(10077,)\n",
            "****************************************\n",
            "New Shapes for windows: 32\n",
            "(8061, 32, 4)\n",
            "(8061,)\n",
            "(2016, 32, 4)\n",
            "(2016,)\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NNoanSIaVwnS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_smooth_size(old_size, batch_size=32):\n",
        "  new_size = old_size - old_size%batch_size\n",
        "  print(\"Smoothed from: [\"+str(old_size)+\"] to: [\"+str(new_size)+\"]\")\n",
        "  return new_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I4bQeYk1V2hy",
        "colab_type": "code",
        "outputId": "3c59b435-b808-4c59-a5e0-309b01cc6c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# MAKE THE DATA SMOOT FOR LSTM MODEL\n",
        "WINDOWS = [WINDOW]\n",
        "BATCH_SIZES = [BATCH_SIZE]\n",
        "for i in range(len(WINDOWS)):\n",
        "  train_smooth_size = get_smooth_size(X_train_list[i].shape[0], batch_size=BATCH_SIZES[i])\n",
        "  test_smooth_size = get_smooth_size(X_test_list[i].shape[0], batch_size=BATCH_SIZES[i])\n",
        "  val_smooth_size = get_smooth_size(X_val_list[i].shape[0], batch_size=BATCH_SIZES[i])\n",
        "\n",
        "  X_train_list[i] = X_train_list[i][:train_smooth_size] \n",
        "  X_test_list[i] = X_test_list[i][:test_smooth_size] \n",
        "  X_val_list[i] = X_val_list[i][:val_smooth_size] \n",
        "\n",
        "  Y_train_list[i] = Y_train_list[i][:train_smooth_size]\n",
        "  Y_test_list[i] = Y_test_list[i][:test_smooth_size]\n",
        "  Y_val_list[i] = Y_val_list[i][:val_smooth_size]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Smoothed from: [8061] to: [8032]\n",
            "Smoothed from: [30] to: [0]\n",
            "Smoothed from: [2016] to: [2016]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "clUkrfzQdcks",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## MODELS"
      ]
    },
    {
      "metadata": {
        "id": "S9RGROfuXUlR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_lstm_model_1(tensor_shape, batch_size=100, hidden_neurons=100):\n",
        "  model = tf.keras.Sequential()\n",
        "# hyperbolic tangent is automatically seleceted as activation function\n",
        "  model.add(layers.LSTM(hidden_neurons, batch_input_shape=(batch_size, tensor_shape[0], tensor_shape[1])))\n",
        "\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(32, activation='relu'))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  \n",
        "  model.add(layers.Dense(1))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1hUh-hbYsvx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_lstm_model_2(tensor_shape, batch_size=100, hidden_neurons=100):\n",
        "  model = tf.keras.Sequential()\n",
        "# hyperbolic tangent is automatically seleceted as activation function\n",
        "  model.add(layers.LSTM(hidden_neurons, batch_input_shape=(batch_size, tensor_shape[0], tensor_shape[1]),return_sequences=True))\n",
        "  model.add(layers.LSTM(hidden_neurons, batch_input_shape=(batch_size, tensor_shape[0], tensor_shape[1])))\n",
        "#  model.add(layers.GlobalAveragePooling1D())\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(32, activation='relu'))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  \n",
        "  model.add(layers.Dense(1))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W2JyUFtRZG6I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TODO: weight initialization and L2 regularization\n",
        "# A linear layer with L2 regularization of factor 0.02\n",
        "\n",
        "\n",
        "def build_lstm_model_3(tensor_shape, batch_size=100, hidden_neurons=100, pooling_size=2):\n",
        "  # L2 norm regularizer\n",
        "  regularizer = tf.keras.regularizers.l2(0.01)\n",
        "  \n",
        "  # XAVIER INITIALIZER\n",
        "  initializer = tf.keras.initializers.glorot_uniform()\n",
        "  \n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "# hyperbolic tangent is automatically seleceted as activation function\n",
        "  model.add(layers.LSTM(hidden_neurons, \n",
        "                        batch_input_shape=(batch_size, tensor_shape[0], tensor_shape[1]),\n",
        "                        recurrent_regularizer=regularizer,\n",
        "                        bias_regularizer=regularizer, \n",
        "                        kernel_regularizer=regularizer,\n",
        "                        recurrent_initializer=initializer,\n",
        "                        kernel_initializer=initializer,\n",
        "                        bias_initializer=initializer\n",
        "                       ))\n",
        "\n",
        "# TODO\n",
        "\n",
        "#  model.add(layers.GlobalAveragePooling1D())\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(32, activation='relu', \n",
        "                         kernel_regularizer=regularizer, \n",
        "                         bias_regularizer=regularizer,\n",
        "                         kernel_initializer=initializer,\n",
        "                         bias_initializer=initializer))\n",
        "  \n",
        "  model.add(layers.Dropout(0.5))\n",
        "  \n",
        "  model.add(layers.Dense(1))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UucKQee-XwUB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_conv1d_model_1(tensor_shape, filters=64, kernel_size=3, pooling_size=2, dropout=0.5):\n",
        "  #todo: maybe add strides\n",
        "  \n",
        "  # initialize the placeholdder\n",
        "  placeholder_input = tf.keras.Input(shape=tensor_shape)\n",
        "  \n",
        "  #------------- FIRST CNN BLOCK: CONV1D & MAXPOOLING ------------- \n",
        "  model = layers.Conv1D(filters=filters, \n",
        "                        kernel_size=kernel_size, \n",
        "                        padding=\"same\",\n",
        "                        input_shape=tensor_shape,\n",
        "                        batch_input_shape=(None, tensor_shape[0], tensor_shape[1]),\n",
        "                        activation='relu')(placeholder_input)\n",
        "  \n",
        "  model = layers.MaxPool1D(pool_size=pooling_size, \n",
        "                           padding=\"same\")(model)\n",
        "  \n",
        "  # CONSIDER:  model = layers.BatchNormalization(model)  \n",
        "  # ------------- Second CNN BLOCK: CONV1D & MAXPOOLING ------------- \n",
        "  model = layers.Conv1D(filters=filters, \n",
        "                        kernel_size=kernel_size, \n",
        "                        padding=\"same\", \n",
        "                        activation='relu')(model)\n",
        "  \n",
        "  model = layers.MaxPool1D(pool_size=pooling_size, \n",
        "                           padding=\"same\")(model)\n",
        "  # ------------- DROPOUT & FLATTEN ------------- \n",
        "  model = layers.Dropout(dropout)(model)\n",
        "  model = layers.Flatten()(model)\n",
        "  \n",
        "  \n",
        "  model = layers.Dense(32, activation='relu')(model)\n",
        "  model = layers.Dropout(dropout)(model)\n",
        "  # not sure if this necessary 1\n",
        "  #model = layers.Activation(activation='relu')(model)\n",
        "  \n",
        "  prediction = layers.Dense(1)(model)\n",
        "  \n",
        "  model = tf.keras.Model(inputs=placeholder_input, outputs=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x4uCh2SedE59",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_conv1d_model_2(tensor_shape, filters=64, kernel_size=3, pooling_size=2, dropout=0.5):\n",
        "  # L2 norm regularizer\n",
        "  regularizer = tf.keras.regularizers.l2(0.01)\n",
        "  \n",
        "  # XAVIER INITIALIZER\n",
        "  initializer = tf.keras.initializers.glorot_uniform()\n",
        "  \n",
        "  # initialize the placeholdder\n",
        "  placeholder_input = tf.keras.Input(shape=tensor_shape)\n",
        "  \n",
        "  #------------- FIRST CNN BLOCK: CONV1D & MAXPOOLING ------------- \n",
        "  model = layers.Conv1D(filters=filters, \n",
        "                        kernel_size=kernel_size, \n",
        "                        padding=\"same\",\n",
        "                        input_shape=tensor_shape,\n",
        "                        batch_input_shape=(None, tensor_shape[0], tensor_shape[1]),\n",
        "                        activation='relu',                         \n",
        "                        kernel_regularizer=regularizer, \n",
        "                        bias_regularizer=regularizer,\n",
        "                        kernel_initializer=initializer,\n",
        "                        bias_initializer=initializer)(placeholder_input)\n",
        "  \n",
        "  model = layers.MaxPool1D(pool_size=pooling_size, \n",
        "                           padding=\"same\")(model)\n",
        "  \n",
        "  # CONSIDER:  model = layers.BatchNormalization(model)  \n",
        "  # ------------- Second CNN BLOCK: CONV1D & MAXPOOLING ------------- \n",
        "  model = layers.Conv1D(filters=filters, \n",
        "                        kernel_size=kernel_size, \n",
        "                        padding=\"same\", \n",
        "                        activation='relu',\n",
        "                        kernel_regularizer=regularizer, \n",
        "                        bias_regularizer=regularizer,\n",
        "                        kernel_initializer=initializer,\n",
        "                        bias_initializer=initializer)(model)\n",
        "  \n",
        "  model = layers.MaxPool1D(pool_size=pooling_size, \n",
        "                           padding=\"same\")(model)\n",
        "  # ------------- DROPOUT & FLATTEN ------------- \n",
        "  model = layers.Dropout(dropout)(model)\n",
        "  model = layers.Flatten()(model)\n",
        "  \n",
        "  \n",
        "  model = layers.Dense(32, activation='relu',\n",
        "                       kernel_regularizer=regularizer, \n",
        "                       bias_regularizer=regularizer,\n",
        "                       kernel_initializer=initializer,\n",
        "                       bias_initializer=initializer)(model)\n",
        "  \n",
        "  model = layers.Dropout(dropout)(model)\n",
        "  # not sure if this necessary 1\n",
        "  #model = layers.Activation(activation='relu')(model)\n",
        "  \n",
        "  prediction = layers.Dense(1)(model)\n",
        "  \n",
        "  model = tf.keras.Model(inputs=placeholder_input, outputs=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_8wIZZUYX-OT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#KERNEL_SIZES = [3,4,5]\n",
        "#FILTER_SIZES = [32, 64, 128]\n",
        "#NEURONS = [100, 200]\n",
        "neuron = 100\n",
        "kernel_size = 3\n",
        "filter_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g0QZlMgWdfbL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# NOTE: model3 seems nice.\n",
        "shape = (X_train_list[i].shape[1], X_train_list[i].shape[2])\n",
        "x1 = build_lstm_model_1(tensor_shape=shape, batch_size=BATCH_SIZE, hidden_neurons=neuron)\n",
        "x2 = build_lstm_model_2(tensor_shape=shape, batch_size=BATCH_SIZE, hidden_neurons=neuron)\n",
        "x3 = build_lstm_model_3(tensor_shape=shape, batch_size=BATCH_SIZE, hidden_neurons=neuron)\n",
        "c1 = build_conv1d_model_1(tensor_shape=shape, filters=filter_size, kernel_size=kernel_size)\n",
        "c2 = build_conv1d_model_2(tensor_shape=shape, filters=filter_size, kernel_size=kernel_size)\n",
        "models = [x1, x2, x3, c1, c2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XuGVZWjAhggD",
        "colab_type": "code",
        "outputId": "8da7dae0-d6ba-4e72-ada7-156660a43769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1921
        }
      },
      "cell_type": "code",
      "source": [
        "for m in models:\n",
        "  print(m.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_4 (LSTM)                (32, 100)                 42000     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_3 (Ba (32, 100)                 400       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (32, 32)                  3232      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (32, 32)                  0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (32, 1)                   33        \n",
            "=================================================================\n",
            "Total params: 45,665\n",
            "Trainable params: 45,465\n",
            "Non-trainable params: 200\n",
            "_________________________________________________________________\n",
            "None\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_5 (LSTM)                (32, 32, 100)             42000     \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (32, 100)                 80400     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_4 (Ba (32, 100)                 400       \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (32, 32)                  3232      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (32, 32)                  0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (32, 1)                   33        \n",
            "=================================================================\n",
            "Total params: 126,065\n",
            "Trainable params: 125,865\n",
            "Non-trainable params: 200\n",
            "_________________________________________________________________\n",
            "None\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_7 (LSTM)                (32, 100)                 42000     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_5 (Ba (32, 100)                 400       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (32, 32)                  3232      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (32, 32)                  0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (32, 1)                   33        \n",
            "=================================================================\n",
            "Total params: 45,665\n",
            "Trainable params: 45,465\n",
            "Non-trainable params: 200\n",
            "_________________________________________________________________\n",
            "None\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 32, 4)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 32, 32)            416       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 16, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 16, 32)            3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 8, 32)             0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 8, 32)             0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 32)                8224      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 11,777\n",
            "Trainable params: 11,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 32, 4)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 32, 32)            416       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1 (None, 16, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 16, 32)            3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1 (None, 8, 32)             0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 8, 32)             0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 32)                8224      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 11,777\n",
            "Trainable params: 11,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uPFqVZhSh-oc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#OPTIMIZER\n",
        "opt_1 = 'adam'\n",
        "opt_2 = tf.keras.optimizers.RMSprop(0.001)\n",
        "opt_3 = tf.keras.optimizers.Adadelta(0.01)\n",
        "#opt = tf.keras.optimizers.RMSprop()\n",
        "\n",
        "loss_func = tf.keras.losses.mean_squared_error\n",
        "metrics =  ['mean_absolute_error', 'mean_squared_error']\n",
        "\n",
        "#tf.train.AdamOptimizer(learning_rate=0.002)\n",
        "\n",
        "# CALLBACKS\n",
        "save_folder = \"/content/gdrive/My Drive/deep_learning/_data/numpy_arrays/\"\n",
        "fp = save_folder+\"d_fx_multi_fcst_1980_2018.hdf5\"\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=30, min_lr=0.000001, verbose=1)\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=fp, verbose=1, save_best_only=True)\n",
        "\n",
        "#COMPILE\n",
        "for m in models:\n",
        "  m.compile(optimizer=opt_3, loss=loss_func, metrics=metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D2g6gGeXjjcq",
        "colab_type": "code",
        "outputId": "5ec5648c-a31a-4acc-9816-f78d6db35b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 20301
        }
      },
      "cell_type": "code",
      "source": [
        "EPOCH_COUNT = 1000 \n",
        "patience=0.1*EPOCH_COUNT\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "\n",
        "histories = []\n",
        "for m in models:\n",
        "  history_m = m.fit(\n",
        "      X_train_list[i], Y_train_list[i], \n",
        "      epochs = 100, batch_size = BATCH_SIZE, verbose=1, \n",
        "      validation_data=(X_val_list[i], Y_val_list[i]),\n",
        "      callbacks=[early_stop, checkpointer])\n",
        "    \n",
        "  histories.append(history_m)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8032 samples, validate on 2016 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.9989 - mean_absolute_error: 0.9163 - mean_squared_error: 0.9989\n",
            "Epoch 00001: val_loss improved from inf to 1.10317, saving model to /content/gdrive/My Drive/deep_learning/_data/numpy_arrays/d_fx_multi_fcst_1980_2018.hdf5\n",
            "8032/8032 [==============================] - 23s 3ms/sample - loss: 0.9974 - mean_absolute_error: 0.9154 - mean_squared_error: 0.9974 - val_loss: 1.1032 - val_mean_absolute_error: 1.0498 - val_mean_squared_error: 1.1032\n",
            "Epoch 2/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.6841 - mean_absolute_error: 0.7397 - mean_squared_error: 0.6841\n",
            "Epoch 00002: val_loss improved from 1.10317 to 0.87868, saving model to /content/gdrive/My Drive/deep_learning/_data/numpy_arrays/d_fx_multi_fcst_1980_2018.hdf5\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.6844 - mean_absolute_error: 0.7397 - mean_squared_error: 0.6844 - val_loss: 0.8787 - val_mean_absolute_error: 0.9341 - val_mean_squared_error: 0.8787\n",
            "Epoch 3/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.6169 - mean_absolute_error: 0.6990 - mean_squared_error: 0.6169\n",
            "Epoch 00003: val_loss improved from 0.87868 to 0.33479, saving model to /content/gdrive/My Drive/deep_learning/_data/numpy_arrays/d_fx_multi_fcst_1980_2018.hdf5\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.6161 - mean_absolute_error: 0.6984 - mean_squared_error: 0.6161 - val_loss: 0.3348 - val_mean_absolute_error: 0.5273 - val_mean_squared_error: 0.3348\n",
            "Epoch 4/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.5560 - mean_absolute_error: 0.6628 - mean_squared_error: 0.5560\n",
            "Epoch 00004: val_loss improved from 0.33479 to 0.25012, saving model to /content/gdrive/My Drive/deep_learning/_data/numpy_arrays/d_fx_multi_fcst_1980_2018.hdf5\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.5557 - mean_absolute_error: 0.6624 - mean_squared_error: 0.5557 - val_loss: 0.2501 - val_mean_absolute_error: 0.4348 - val_mean_squared_error: 0.2501\n",
            "Epoch 5/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4980 - mean_absolute_error: 0.6233 - mean_squared_error: 0.4980\n",
            "Epoch 00005: val_loss improved from 0.25012 to 0.12736, saving model to /content/gdrive/My Drive/deep_learning/_data/numpy_arrays/d_fx_multi_fcst_1980_2018.hdf5\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.4975 - mean_absolute_error: 0.6228 - mean_squared_error: 0.4975 - val_loss: 0.1274 - val_mean_absolute_error: 0.3002 - val_mean_squared_error: 0.1274\n",
            "Epoch 6/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4373 - mean_absolute_error: 0.5814 - mean_squared_error: 0.4373\n",
            "Epoch 00006: val_loss improved from 0.12736 to 0.05170, saving model to /content/gdrive/My Drive/deep_learning/_data/numpy_arrays/d_fx_multi_fcst_1980_2018.hdf5\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.4374 - mean_absolute_error: 0.5815 - mean_squared_error: 0.4374 - val_loss: 0.0517 - val_mean_absolute_error: 0.1974 - val_mean_squared_error: 0.0517\n",
            "Epoch 7/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.3627 - mean_absolute_error: 0.5234 - mean_squared_error: 0.3627\n",
            "Epoch 00007: val_loss did not improve from 0.05170\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.3625 - mean_absolute_error: 0.5233 - mean_squared_error: 0.3625 - val_loss: 0.1193 - val_mean_absolute_error: 0.3170 - val_mean_squared_error: 0.1193\n",
            "Epoch 8/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.3018 - mean_absolute_error: 0.4663 - mean_squared_error: 0.3018\n",
            "Epoch 00008: val_loss did not improve from 0.05170\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.3019 - mean_absolute_error: 0.4666 - mean_squared_error: 0.3019 - val_loss: 0.3179 - val_mean_absolute_error: 0.5576 - val_mean_squared_error: 0.3179\n",
            "Epoch 9/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.2595 - mean_absolute_error: 0.4256 - mean_squared_error: 0.2595\n",
            "Epoch 00009: val_loss did not improve from 0.05170\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.2593 - mean_absolute_error: 0.4254 - mean_squared_error: 0.2593 - val_loss: 0.0703 - val_mean_absolute_error: 0.2563 - val_mean_squared_error: 0.0703\n",
            "Epoch 10/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.2302 - mean_absolute_error: 0.3928 - mean_squared_error: 0.2302\n",
            "Epoch 00010: val_loss improved from 0.05170 to 0.00844, saving model to /content/gdrive/My Drive/deep_learning/_data/numpy_arrays/d_fx_multi_fcst_1980_2018.hdf5\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.2302 - mean_absolute_error: 0.3928 - mean_squared_error: 0.2302 - val_loss: 0.0084 - val_mean_absolute_error: 0.0770 - val_mean_squared_error: 0.0084\n",
            "Epoch 11/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.2191 - mean_absolute_error: 0.3829 - mean_squared_error: 0.2191\n",
            "Epoch 00011: val_loss did not improve from 0.00844\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.2191 - mean_absolute_error: 0.3829 - mean_squared_error: 0.2191 - val_loss: 0.0920 - val_mean_absolute_error: 0.2958 - val_mean_squared_error: 0.0920\n",
            "Epoch 12/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.2127 - mean_absolute_error: 0.3771 - mean_squared_error: 0.2127\n",
            "Epoch 00012: val_loss did not improve from 0.00844\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.2127 - mean_absolute_error: 0.3770 - mean_squared_error: 0.2127 - val_loss: 0.1250 - val_mean_absolute_error: 0.3485 - val_mean_squared_error: 0.1250\n",
            "Epoch 13/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.2081 - mean_absolute_error: 0.3702 - mean_squared_error: 0.2081\n",
            "Epoch 00013: val_loss did not improve from 0.00844\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.2081 - mean_absolute_error: 0.3703 - mean_squared_error: 0.2081 - val_loss: 0.1424 - val_mean_absolute_error: 0.3718 - val_mean_squared_error: 0.1424\n",
            "Epoch 14/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.2044 - mean_absolute_error: 0.3681 - mean_squared_error: 0.2044\n",
            "Epoch 00014: val_loss did not improve from 0.00844\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.2042 - mean_absolute_error: 0.3678 - mean_squared_error: 0.2042 - val_loss: 0.0513 - val_mean_absolute_error: 0.2215 - val_mean_squared_error: 0.0513\n",
            "Epoch 15/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.2080 - mean_absolute_error: 0.3701 - mean_squared_error: 0.2080\n",
            "Epoch 00015: val_loss did not improve from 0.00844\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.2077 - mean_absolute_error: 0.3698 - mean_squared_error: 0.2077 - val_loss: 0.0419 - val_mean_absolute_error: 0.1958 - val_mean_squared_error: 0.0419\n",
            "Epoch 16/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1869 - mean_absolute_error: 0.3497 - mean_squared_error: 0.1869\n",
            "Epoch 00016: val_loss did not improve from 0.00844\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1872 - mean_absolute_error: 0.3499 - mean_squared_error: 0.1872 - val_loss: 0.2230 - val_mean_absolute_error: 0.4693 - val_mean_squared_error: 0.2230\n",
            "Epoch 17/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1900 - mean_absolute_error: 0.3526 - mean_squared_error: 0.1900\n",
            "Epoch 00017: val_loss improved from 0.00844 to 0.00282, saving model to /content/gdrive/My Drive/deep_learning/_data/numpy_arrays/d_fx_multi_fcst_1980_2018.hdf5\n",
            "8032/8032 [==============================] - 22s 3ms/sample - loss: 0.1895 - mean_absolute_error: 0.3521 - mean_squared_error: 0.1895 - val_loss: 0.0028 - val_mean_absolute_error: 0.0433 - val_mean_squared_error: 0.0028\n",
            "Epoch 18/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1864 - mean_absolute_error: 0.3509 - mean_squared_error: 0.1864\n",
            "Epoch 00018: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.1868 - mean_absolute_error: 0.3514 - mean_squared_error: 0.1868 - val_loss: 0.3510 - val_mean_absolute_error: 0.5889 - val_mean_squared_error: 0.3510\n",
            "Epoch 19/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1851 - mean_absolute_error: 0.3475 - mean_squared_error: 0.1851\n",
            "Epoch 00019: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.1850 - mean_absolute_error: 0.3474 - mean_squared_error: 0.1850 - val_loss: 0.0884 - val_mean_absolute_error: 0.2925 - val_mean_squared_error: 0.0884\n",
            "Epoch 20/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1799 - mean_absolute_error: 0.3435 - mean_squared_error: 0.1799\n",
            "Epoch 00020: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1798 - mean_absolute_error: 0.3434 - mean_squared_error: 0.1798 - val_loss: 0.0594 - val_mean_absolute_error: 0.2398 - val_mean_squared_error: 0.0594\n",
            "Epoch 21/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1821 - mean_absolute_error: 0.3460 - mean_squared_error: 0.1821\n",
            "Epoch 00021: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1824 - mean_absolute_error: 0.3463 - mean_squared_error: 0.1824 - val_loss: 0.0494 - val_mean_absolute_error: 0.2190 - val_mean_squared_error: 0.0494\n",
            "Epoch 22/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1785 - mean_absolute_error: 0.3427 - mean_squared_error: 0.1785\n",
            "Epoch 00022: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.1784 - mean_absolute_error: 0.3426 - mean_squared_error: 0.1784 - val_loss: 0.0902 - val_mean_absolute_error: 0.2979 - val_mean_squared_error: 0.0902\n",
            "Epoch 23/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1733 - mean_absolute_error: 0.3356 - mean_squared_error: 0.1733\n",
            "Epoch 00023: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.1731 - mean_absolute_error: 0.3355 - mean_squared_error: 0.1731 - val_loss: 0.0801 - val_mean_absolute_error: 0.2806 - val_mean_squared_error: 0.0801\n",
            "Epoch 24/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1720 - mean_absolute_error: 0.3337 - mean_squared_error: 0.1720\n",
            "Epoch 00024: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.1720 - mean_absolute_error: 0.3337 - mean_squared_error: 0.1720 - val_loss: 0.1982 - val_mean_absolute_error: 0.4434 - val_mean_squared_error: 0.1982\n",
            "Epoch 25/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1719 - mean_absolute_error: 0.3351 - mean_squared_error: 0.1719\n",
            "Epoch 00025: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1720 - mean_absolute_error: 0.3352 - mean_squared_error: 0.1720 - val_loss: 0.0157 - val_mean_absolute_error: 0.1178 - val_mean_squared_error: 0.0157\n",
            "Epoch 26/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1653 - mean_absolute_error: 0.3283 - mean_squared_error: 0.1653\n",
            "Epoch 00026: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1655 - mean_absolute_error: 0.3285 - mean_squared_error: 0.1655 - val_loss: 0.0686 - val_mean_absolute_error: 0.2596 - val_mean_squared_error: 0.0686\n",
            "Epoch 27/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1675 - mean_absolute_error: 0.3287 - mean_squared_error: 0.1675\n",
            "Epoch 00027: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1675 - mean_absolute_error: 0.3288 - mean_squared_error: 0.1675 - val_loss: 0.2196 - val_mean_absolute_error: 0.4671 - val_mean_squared_error: 0.2196\n",
            "Epoch 28/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1633 - mean_absolute_error: 0.3256 - mean_squared_error: 0.1633\n",
            "Epoch 00028: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1634 - mean_absolute_error: 0.3257 - mean_squared_error: 0.1634 - val_loss: 0.0822 - val_mean_absolute_error: 0.2848 - val_mean_squared_error: 0.0822\n",
            "Epoch 29/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1629 - mean_absolute_error: 0.3263 - mean_squared_error: 0.1629\n",
            "Epoch 00029: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1627 - mean_absolute_error: 0.3261 - mean_squared_error: 0.1627 - val_loss: 0.1051 - val_mean_absolute_error: 0.3228 - val_mean_squared_error: 0.1051\n",
            "Epoch 30/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1565 - mean_absolute_error: 0.3182 - mean_squared_error: 0.1565\n",
            "Epoch 00030: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1565 - mean_absolute_error: 0.3183 - mean_squared_error: 0.1565 - val_loss: 0.1079 - val_mean_absolute_error: 0.3266 - val_mean_squared_error: 0.1079\n",
            "Epoch 31/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1555 - mean_absolute_error: 0.3192 - mean_squared_error: 0.1555\n",
            "Epoch 00031: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1555 - mean_absolute_error: 0.3192 - mean_squared_error: 0.1555 - val_loss: 0.0632 - val_mean_absolute_error: 0.2498 - val_mean_squared_error: 0.0632\n",
            "Epoch 32/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1512 - mean_absolute_error: 0.3146 - mean_squared_error: 0.1512\n",
            "Epoch 00032: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.1513 - mean_absolute_error: 0.3147 - mean_squared_error: 0.1513 - val_loss: 0.0724 - val_mean_absolute_error: 0.2676 - val_mean_squared_error: 0.0724\n",
            "Epoch 33/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1483 - mean_absolute_error: 0.3097 - mean_squared_error: 0.1483\n",
            "Epoch 00033: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1483 - mean_absolute_error: 0.3096 - mean_squared_error: 0.1483 - val_loss: 0.0587 - val_mean_absolute_error: 0.2409 - val_mean_squared_error: 0.0587\n",
            "Epoch 34/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1454 - mean_absolute_error: 0.3083 - mean_squared_error: 0.1454\n",
            "Epoch 00034: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1452 - mean_absolute_error: 0.3081 - mean_squared_error: 0.1452 - val_loss: 0.0167 - val_mean_absolute_error: 0.1259 - val_mean_squared_error: 0.0167\n",
            "Epoch 35/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1413 - mean_absolute_error: 0.3026 - mean_squared_error: 0.1413\n",
            "Epoch 00035: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1415 - mean_absolute_error: 0.3029 - mean_squared_error: 0.1415 - val_loss: 0.2904 - val_mean_absolute_error: 0.5365 - val_mean_squared_error: 0.2904\n",
            "Epoch 36/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1401 - mean_absolute_error: 0.3010 - mean_squared_error: 0.1401\n",
            "Epoch 00036: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1402 - mean_absolute_error: 0.3011 - mean_squared_error: 0.1402 - val_loss: 0.1366 - val_mean_absolute_error: 0.3688 - val_mean_squared_error: 0.1366\n",
            "Epoch 37/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1400 - mean_absolute_error: 0.2995 - mean_squared_error: 0.1400\n",
            "Epoch 00037: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.1400 - mean_absolute_error: 0.2995 - mean_squared_error: 0.1400 - val_loss: 0.2124 - val_mean_absolute_error: 0.4587 - val_mean_squared_error: 0.2124\n",
            "Train on 8032 samples, validate on 2016 samples\n",
            "Epoch 1/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.7518 - mean_absolute_error: 0.7882 - mean_squared_error: 0.7518\n",
            "Epoch 00001: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 40s 5ms/sample - loss: 0.7509 - mean_absolute_error: 0.7875 - mean_squared_error: 0.7509 - val_loss: 1.0356 - val_mean_absolute_error: 1.0170 - val_mean_squared_error: 1.0356\n",
            "Epoch 2/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.5596 - mean_absolute_error: 0.6612 - mean_squared_error: 0.5596\n",
            "Epoch 00002: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.5597 - mean_absolute_error: 0.6611 - mean_squared_error: 0.5597 - val_loss: 0.6904 - val_mean_absolute_error: 0.8233 - val_mean_squared_error: 0.6904\n",
            "Epoch 3/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.5136 - mean_absolute_error: 0.6311 - mean_squared_error: 0.5136\n",
            "Epoch 00003: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.5129 - mean_absolute_error: 0.6306 - mean_squared_error: 0.5129 - val_loss: 0.2410 - val_mean_absolute_error: 0.4123 - val_mean_squared_error: 0.2410\n",
            "Epoch 4/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4601 - mean_absolute_error: 0.5953 - mean_squared_error: 0.4601\n",
            "Epoch 00004: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.4601 - mean_absolute_error: 0.5954 - mean_squared_error: 0.4601 - val_loss: 0.1387 - val_mean_absolute_error: 0.3336 - val_mean_squared_error: 0.1387\n",
            "Epoch 5/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.3938 - mean_absolute_error: 0.5498 - mean_squared_error: 0.3938\n",
            "Epoch 00005: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.3931 - mean_absolute_error: 0.5492 - mean_squared_error: 0.3931 - val_loss: 0.5160 - val_mean_absolute_error: 0.6358 - val_mean_squared_error: 0.5160\n",
            "Epoch 6/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.3164 - mean_absolute_error: 0.4888 - mean_squared_error: 0.3164\n",
            "Epoch 00006: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.3162 - mean_absolute_error: 0.4887 - mean_squared_error: 0.3162 - val_loss: 0.9509 - val_mean_absolute_error: 0.9586 - val_mean_squared_error: 0.9509\n",
            "Epoch 7/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.2451 - mean_absolute_error: 0.4195 - mean_squared_error: 0.2451\n",
            "Epoch 00007: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.2449 - mean_absolute_error: 0.4193 - mean_squared_error: 0.2449 - val_loss: 1.0208 - val_mean_absolute_error: 1.0076 - val_mean_squared_error: 1.0208\n",
            "Epoch 8/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.2090 - mean_absolute_error: 0.3771 - mean_squared_error: 0.2090\n",
            "Epoch 00008: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.2088 - mean_absolute_error: 0.3770 - mean_squared_error: 0.2088 - val_loss: 0.0855 - val_mean_absolute_error: 0.2743 - val_mean_squared_error: 0.0855\n",
            "Epoch 9/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1907 - mean_absolute_error: 0.3584 - mean_squared_error: 0.1907\n",
            "Epoch 00009: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1906 - mean_absolute_error: 0.3584 - mean_squared_error: 0.1906 - val_loss: 0.0086 - val_mean_absolute_error: 0.0768 - val_mean_squared_error: 0.0086\n",
            "Epoch 10/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1781 - mean_absolute_error: 0.3425 - mean_squared_error: 0.1781\n",
            "Epoch 00010: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1782 - mean_absolute_error: 0.3426 - mean_squared_error: 0.1782 - val_loss: 0.0577 - val_mean_absolute_error: 0.2366 - val_mean_squared_error: 0.0577\n",
            "Epoch 11/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1659 - mean_absolute_error: 0.3301 - mean_squared_error: 0.1659\n",
            "Epoch 00011: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1658 - mean_absolute_error: 0.3300 - mean_squared_error: 0.1658 - val_loss: 0.0084 - val_mean_absolute_error: 0.0764 - val_mean_squared_error: 0.0084\n",
            "Epoch 12/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1606 - mean_absolute_error: 0.3229 - mean_squared_error: 0.1606\n",
            "Epoch 00012: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1608 - mean_absolute_error: 0.3231 - mean_squared_error: 0.1608 - val_loss: 0.0601 - val_mean_absolute_error: 0.2385 - val_mean_squared_error: 0.0601\n",
            "Epoch 13/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1580 - mean_absolute_error: 0.3224 - mean_squared_error: 0.1580\n",
            "Epoch 00013: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1581 - mean_absolute_error: 0.3225 - mean_squared_error: 0.1581 - val_loss: 0.0618 - val_mean_absolute_error: 0.2425 - val_mean_squared_error: 0.0618\n",
            "Epoch 14/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1508 - mean_absolute_error: 0.3146 - mean_squared_error: 0.1508\n",
            "Epoch 00014: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1505 - mean_absolute_error: 0.3143 - mean_squared_error: 0.1505 - val_loss: 0.0169 - val_mean_absolute_error: 0.1213 - val_mean_squared_error: 0.0169\n",
            "Epoch 15/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1529 - mean_absolute_error: 0.3171 - mean_squared_error: 0.1529\n",
            "Epoch 00015: val_loss did not improve from 0.00282\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1530 - mean_absolute_error: 0.3172 - mean_squared_error: 0.1530 - val_loss: 0.2524 - val_mean_absolute_error: 0.5003 - val_mean_squared_error: 0.2524\n",
            "Epoch 16/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1535 - mean_absolute_error: 0.3163 - mean_squared_error: 0.1535\n",
            "Epoch 00016: val_loss improved from 0.00282 to 0.00143, saving model to /content/gdrive/My Drive/deep_learning/_data/numpy_arrays/d_fx_multi_fcst_1980_2018.hdf5\n",
            "8032/8032 [==============================] - 40s 5ms/sample - loss: 0.1533 - mean_absolute_error: 0.3161 - mean_squared_error: 0.1533 - val_loss: 0.0014 - val_mean_absolute_error: 0.0283 - val_mean_squared_error: 0.0014\n",
            "Epoch 17/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1502 - mean_absolute_error: 0.3114 - mean_squared_error: 0.1502\n",
            "Epoch 00017: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1501 - mean_absolute_error: 0.3114 - mean_squared_error: 0.1501 - val_loss: 0.1225 - val_mean_absolute_error: 0.3329 - val_mean_squared_error: 0.1225\n",
            "Epoch 18/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1412 - mean_absolute_error: 0.3030 - mean_squared_error: 0.1412\n",
            "Epoch 00018: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1412 - mean_absolute_error: 0.3031 - mean_squared_error: 0.1412 - val_loss: 0.0086 - val_mean_absolute_error: 0.0803 - val_mean_squared_error: 0.0086\n",
            "Epoch 19/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1433 - mean_absolute_error: 0.3047 - mean_squared_error: 0.1433\n",
            "Epoch 00019: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1432 - mean_absolute_error: 0.3045 - mean_squared_error: 0.1432 - val_loss: 0.0052 - val_mean_absolute_error: 0.0615 - val_mean_squared_error: 0.0052\n",
            "Epoch 20/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1379 - mean_absolute_error: 0.2988 - mean_squared_error: 0.1379\n",
            "Epoch 00020: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1377 - mean_absolute_error: 0.2985 - mean_squared_error: 0.1377 - val_loss: 0.1188 - val_mean_absolute_error: 0.3417 - val_mean_squared_error: 0.1188\n",
            "Epoch 21/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1399 - mean_absolute_error: 0.3006 - mean_squared_error: 0.1399\n",
            "Epoch 00021: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1398 - mean_absolute_error: 0.3005 - mean_squared_error: 0.1398 - val_loss: 0.1764 - val_mean_absolute_error: 0.4187 - val_mean_squared_error: 0.1764\n",
            "Epoch 22/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1322 - mean_absolute_error: 0.2925 - mean_squared_error: 0.1322\n",
            "Epoch 00022: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1322 - mean_absolute_error: 0.2925 - mean_squared_error: 0.1322 - val_loss: 0.1285 - val_mean_absolute_error: 0.3531 - val_mean_squared_error: 0.1285\n",
            "Epoch 23/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1334 - mean_absolute_error: 0.2926 - mean_squared_error: 0.1334\n",
            "Epoch 00023: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1333 - mean_absolute_error: 0.2925 - mean_squared_error: 0.1333 - val_loss: 0.1444 - val_mean_absolute_error: 0.3714 - val_mean_squared_error: 0.1444\n",
            "Epoch 24/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1300 - mean_absolute_error: 0.2895 - mean_squared_error: 0.1300\n",
            "Epoch 00024: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1301 - mean_absolute_error: 0.2896 - mean_squared_error: 0.1301 - val_loss: 0.0198 - val_mean_absolute_error: 0.1386 - val_mean_squared_error: 0.0198\n",
            "Epoch 25/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1266 - mean_absolute_error: 0.2853 - mean_squared_error: 0.1266\n",
            "Epoch 00025: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1268 - mean_absolute_error: 0.2855 - mean_squared_error: 0.1268 - val_loss: 0.1883 - val_mean_absolute_error: 0.4284 - val_mean_squared_error: 0.1883\n",
            "Epoch 26/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1251 - mean_absolute_error: 0.2835 - mean_squared_error: 0.1251\n",
            "Epoch 00026: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1251 - mean_absolute_error: 0.2835 - mean_squared_error: 0.1251 - val_loss: 0.0720 - val_mean_absolute_error: 0.2662 - val_mean_squared_error: 0.0720\n",
            "Epoch 27/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1251 - mean_absolute_error: 0.2839 - mean_squared_error: 0.1251\n",
            "Epoch 00027: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1250 - mean_absolute_error: 0.2838 - mean_squared_error: 0.1250 - val_loss: 0.0513 - val_mean_absolute_error: 0.2223 - val_mean_squared_error: 0.0513\n",
            "Epoch 28/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1238 - mean_absolute_error: 0.2822 - mean_squared_error: 0.1238\n",
            "Epoch 00028: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1238 - mean_absolute_error: 0.2821 - mean_squared_error: 0.1238 - val_loss: 0.1648 - val_mean_absolute_error: 0.4043 - val_mean_squared_error: 0.1648\n",
            "Epoch 29/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1199 - mean_absolute_error: 0.2775 - mean_squared_error: 0.1199\n",
            "Epoch 00029: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1200 - mean_absolute_error: 0.2775 - mean_squared_error: 0.1200 - val_loss: 0.0211 - val_mean_absolute_error: 0.1380 - val_mean_squared_error: 0.0211\n",
            "Epoch 30/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1145 - mean_absolute_error: 0.2714 - mean_squared_error: 0.1145\n",
            "Epoch 00030: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1142 - mean_absolute_error: 0.2711 - mean_squared_error: 0.1142 - val_loss: 0.1425 - val_mean_absolute_error: 0.3757 - val_mean_squared_error: 0.1425\n",
            "Epoch 31/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1157 - mean_absolute_error: 0.2719 - mean_squared_error: 0.1157\n",
            "Epoch 00031: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1157 - mean_absolute_error: 0.2718 - mean_squared_error: 0.1157 - val_loss: 0.0122 - val_mean_absolute_error: 0.1067 - val_mean_squared_error: 0.0122\n",
            "Epoch 32/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1139 - mean_absolute_error: 0.2697 - mean_squared_error: 0.1139\n",
            "Epoch 00032: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1140 - mean_absolute_error: 0.2698 - mean_squared_error: 0.1140 - val_loss: 0.0370 - val_mean_absolute_error: 0.1897 - val_mean_squared_error: 0.0370\n",
            "Epoch 33/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1115 - mean_absolute_error: 0.2684 - mean_squared_error: 0.1115\n",
            "Epoch 00033: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1115 - mean_absolute_error: 0.2685 - mean_squared_error: 0.1115 - val_loss: 0.1162 - val_mean_absolute_error: 0.3395 - val_mean_squared_error: 0.1162\n",
            "Epoch 34/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1108 - mean_absolute_error: 0.2666 - mean_squared_error: 0.1108\n",
            "Epoch 00034: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1107 - mean_absolute_error: 0.2663 - mean_squared_error: 0.1107 - val_loss: 0.1515 - val_mean_absolute_error: 0.3885 - val_mean_squared_error: 0.1515\n",
            "Epoch 35/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1043 - mean_absolute_error: 0.2578 - mean_squared_error: 0.1043\n",
            "Epoch 00035: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1043 - mean_absolute_error: 0.2579 - mean_squared_error: 0.1043 - val_loss: 0.1411 - val_mean_absolute_error: 0.3732 - val_mean_squared_error: 0.1411\n",
            "Epoch 36/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1015 - mean_absolute_error: 0.2542 - mean_squared_error: 0.1015\n",
            "Epoch 00036: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 39s 5ms/sample - loss: 0.1013 - mean_absolute_error: 0.2540 - mean_squared_error: 0.1013 - val_loss: 0.1599 - val_mean_absolute_error: 0.3989 - val_mean_squared_error: 0.1599\n",
            "Train on 8032 samples, validate on 2016 samples\n",
            "Epoch 1/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 4.1525 - mean_absolute_error: 0.8935 - mean_squared_error: 0.9836\n",
            "Epoch 00001: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 4.1519 - mean_absolute_error: 0.8934 - mean_squared_error: 0.9832 - val_loss: 4.4367 - val_mean_absolute_error: 1.1381 - val_mean_squared_error: 1.3010\n",
            "Epoch 2/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 3.8173 - mean_absolute_error: 0.7481 - mean_squared_error: 0.7177\n",
            "Epoch 00002: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 3.8168 - mean_absolute_error: 0.7480 - mean_squared_error: 0.7174 - val_loss: 3.5372 - val_mean_absolute_error: 0.6886 - val_mean_squared_error: 0.4748\n",
            "Epoch 3/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 3.6076 - mean_absolute_error: 0.6720 - mean_squared_error: 0.5820\n",
            "Epoch 00003: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 3.6079 - mean_absolute_error: 0.6720 - mean_squared_error: 0.5825 - val_loss: 3.0338 - val_mean_absolute_error: 0.1930 - val_mean_squared_error: 0.0456\n",
            "Epoch 4/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 3.4146 - mean_absolute_error: 0.5932 - mean_squared_error: 0.4623\n",
            "Epoch 00004: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 3.4149 - mean_absolute_error: 0.5936 - mean_squared_error: 0.4628 - val_loss: 2.9814 - val_mean_absolute_error: 0.2000 - val_mean_squared_error: 0.0651\n",
            "Epoch 5/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 3.2517 - mean_absolute_error: 0.5214 - mean_squared_error: 0.3698\n",
            "Epoch 00005: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 3.2513 - mean_absolute_error: 0.5211 - mean_squared_error: 0.3696 - val_loss: 2.9060 - val_mean_absolute_error: 0.1958 - val_mean_squared_error: 0.0590\n",
            "Epoch 6/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 3.1124 - mean_absolute_error: 0.4608 - mean_squared_error: 0.2987\n",
            "Epoch 00006: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 3.1121 - mean_absolute_error: 0.4607 - mean_squared_error: 0.2984 - val_loss: 2.8113 - val_mean_absolute_error: 0.1578 - val_mean_squared_error: 0.0306\n",
            "Epoch 7/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 3.0114 - mean_absolute_error: 0.4223 - mean_squared_error: 0.2623\n",
            "Epoch 00007: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 3.0111 - mean_absolute_error: 0.4222 - mean_squared_error: 0.2621 - val_loss: 2.7492 - val_mean_absolute_error: 0.1588 - val_mean_squared_error: 0.0317\n",
            "Epoch 8/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.9214 - mean_absolute_error: 0.3962 - mean_squared_error: 0.2346\n",
            "Epoch 00008: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 2.9212 - mean_absolute_error: 0.3961 - mean_squared_error: 0.2345 - val_loss: 2.6762 - val_mean_absolute_error: 0.1118 - val_mean_squared_error: 0.0201\n",
            "Epoch 9/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.8427 - mean_absolute_error: 0.3747 - mean_squared_error: 0.2163\n",
            "Epoch 00009: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 2.8425 - mean_absolute_error: 0.3747 - mean_squared_error: 0.2163 - val_loss: 2.6455 - val_mean_absolute_error: 0.2101 - val_mean_squared_error: 0.0490\n",
            "Epoch 10/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.7762 - mean_absolute_error: 0.3676 - mean_squared_error: 0.2088\n",
            "Epoch 00010: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 2.7760 - mean_absolute_error: 0.3675 - mean_squared_error: 0.2086 - val_loss: 2.8628 - val_mean_absolute_error: 0.5638 - val_mean_squared_error: 0.3246\n",
            "Epoch 11/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.7030 - mean_absolute_error: 0.3536 - mean_squared_error: 0.1928\n",
            "Epoch 00011: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 2.7029 - mean_absolute_error: 0.3535 - mean_squared_error: 0.1929 - val_loss: 2.5515 - val_mean_absolute_error: 0.2482 - val_mean_squared_error: 0.0698\n",
            "Epoch 12/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.6371 - mean_absolute_error: 0.3439 - mean_squared_error: 0.1830\n",
            "Epoch 00012: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 2.6374 - mean_absolute_error: 0.3442 - mean_squared_error: 0.1833 - val_loss: 2.5253 - val_mean_absolute_error: 0.3087 - val_mean_squared_error: 0.0989\n",
            "Epoch 13/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.5748 - mean_absolute_error: 0.3367 - mean_squared_error: 0.1757\n",
            "Epoch 00013: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 2.5746 - mean_absolute_error: 0.3367 - mean_squared_error: 0.1756 - val_loss: 2.4215 - val_mean_absolute_error: 0.2151 - val_mean_squared_error: 0.0501\n",
            "Epoch 14/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.5169 - mean_absolute_error: 0.3333 - mean_squared_error: 0.1721\n",
            "Epoch 00014: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 2.5165 - mean_absolute_error: 0.3330 - mean_squared_error: 0.1718 - val_loss: 2.5478 - val_mean_absolute_error: 0.4768 - val_mean_squared_error: 0.2296\n",
            "Epoch 15/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.4560 - mean_absolute_error: 0.3245 - mean_squared_error: 0.1638\n",
            "Epoch 00015: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 2.4559 - mean_absolute_error: 0.3245 - mean_squared_error: 0.1638 - val_loss: 2.3440 - val_mean_absolute_error: 0.2742 - val_mean_squared_error: 0.0778\n",
            "Epoch 16/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.3941 - mean_absolute_error: 0.3146 - mean_squared_error: 0.1534\n",
            "Epoch 00016: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 2.3940 - mean_absolute_error: 0.3145 - mean_squared_error: 0.1533 - val_loss: 2.2938 - val_mean_absolute_error: 0.2770 - val_mean_squared_error: 0.0790\n",
            "Epoch 17/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.3469 - mean_absolute_error: 0.3181 - mean_squared_error: 0.1568\n",
            "Epoch 00017: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 2.3467 - mean_absolute_error: 0.3180 - mean_squared_error: 0.1568 - val_loss: 2.3442 - val_mean_absolute_error: 0.4211 - val_mean_squared_error: 0.1791\n",
            "Epoch 18/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.2864 - mean_absolute_error: 0.3061 - mean_squared_error: 0.1453\n",
            "Epoch 00018: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 2.2862 - mean_absolute_error: 0.3061 - mean_squared_error: 0.1452 - val_loss: 2.2240 - val_mean_absolute_error: 0.3245 - val_mean_squared_error: 0.1074\n",
            "Epoch 19/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.2367 - mean_absolute_error: 0.3051 - mean_squared_error: 0.1442\n",
            "Epoch 00019: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 2.2364 - mean_absolute_error: 0.3049 - mean_squared_error: 0.1440 - val_loss: 2.0995 - val_mean_absolute_error: 0.1701 - val_mean_squared_error: 0.0311\n",
            "Epoch 20/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.1871 - mean_absolute_error: 0.3037 - mean_squared_error: 0.1420\n",
            "Epoch 00020: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 2.1870 - mean_absolute_error: 0.3038 - mean_squared_error: 0.1420 - val_loss: 2.0519 - val_mean_absolute_error: 0.1677 - val_mean_squared_error: 0.0304\n",
            "Epoch 21/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.1335 - mean_absolute_error: 0.2953 - mean_squared_error: 0.1351\n",
            "Epoch 00021: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 2.1336 - mean_absolute_error: 0.2955 - mean_squared_error: 0.1353 - val_loss: 2.0349 - val_mean_absolute_error: 0.2411 - val_mean_squared_error: 0.0598\n",
            "Epoch 22/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.0820 - mean_absolute_error: 0.2889 - mean_squared_error: 0.1301\n",
            "Epoch 00022: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 2.0820 - mean_absolute_error: 0.2890 - mean_squared_error: 0.1301 - val_loss: 1.9825 - val_mean_absolute_error: 0.2262 - val_mean_squared_error: 0.0539\n",
            "Epoch 23/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 2.0343 - mean_absolute_error: 0.2877 - mean_squared_error: 0.1284\n",
            "Epoch 00023: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 2.0346 - mean_absolute_error: 0.2881 - mean_squared_error: 0.1287 - val_loss: 1.9066 - val_mean_absolute_error: 0.1450 - val_mean_squared_error: 0.0235\n",
            "Epoch 24/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.9872 - mean_absolute_error: 0.2844 - mean_squared_error: 0.1260\n",
            "Epoch 00024: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.9869 - mean_absolute_error: 0.2841 - mean_squared_error: 0.1258 - val_loss: 1.8669 - val_mean_absolute_error: 0.1611 - val_mean_squared_error: 0.0276\n",
            "Epoch 25/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.9444 - mean_absolute_error: 0.2841 - mean_squared_error: 0.1262\n",
            "Epoch 00025: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.9444 - mean_absolute_error: 0.2842 - mean_squared_error: 0.1262 - val_loss: 1.8864 - val_mean_absolute_error: 0.2964 - val_mean_squared_error: 0.0896\n",
            "Epoch 26/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.8988 - mean_absolute_error: 0.2816 - mean_squared_error: 0.1229\n",
            "Epoch 00026: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.8986 - mean_absolute_error: 0.2814 - mean_squared_error: 0.1227 - val_loss: 1.9244 - val_mean_absolute_error: 0.4096 - val_mean_squared_error: 0.1693\n",
            "Epoch 27/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.8593 - mean_absolute_error: 0.2828 - mean_squared_error: 0.1245\n",
            "Epoch 00027: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.8591 - mean_absolute_error: 0.2825 - mean_squared_error: 0.1243 - val_loss: 1.8160 - val_mean_absolute_error: 0.3167 - val_mean_squared_error: 0.1013\n",
            "Epoch 28/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.8134 - mean_absolute_error: 0.2741 - mean_squared_error: 0.1183\n",
            "Epoch 00028: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.8134 - mean_absolute_error: 0.2741 - mean_squared_error: 0.1183 - val_loss: 1.8092 - val_mean_absolute_error: 0.3640 - val_mean_squared_error: 0.1337\n",
            "Epoch 29/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.7709 - mean_absolute_error: 0.2705 - mean_squared_error: 0.1144\n",
            "Epoch 00029: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.7711 - mean_absolute_error: 0.2709 - mean_squared_error: 0.1147 - val_loss: 1.6659 - val_mean_absolute_error: 0.1653 - val_mean_squared_error: 0.0284\n",
            "Epoch 30/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.7348 - mean_absolute_error: 0.2709 - mean_squared_error: 0.1159\n",
            "Epoch 00030: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.7347 - mean_absolute_error: 0.2708 - mean_squared_error: 0.1158 - val_loss: 1.6379 - val_mean_absolute_error: 0.1892 - val_mean_squared_error: 0.0376\n",
            "Epoch 31/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.6964 - mean_absolute_error: 0.2689 - mean_squared_error: 0.1141\n",
            "Epoch 00031: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.6963 - mean_absolute_error: 0.2689 - mean_squared_error: 0.1141 - val_loss: 1.6046 - val_mean_absolute_error: 0.1984 - val_mean_squared_error: 0.0404\n",
            "Epoch 32/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.6582 - mean_absolute_error: 0.2673 - mean_squared_error: 0.1115\n",
            "Epoch 00032: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.6581 - mean_absolute_error: 0.2673 - mean_squared_error: 0.1115 - val_loss: 1.5885 - val_mean_absolute_error: 0.2409 - val_mean_squared_error: 0.0595\n",
            "Epoch 33/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.6186 - mean_absolute_error: 0.2614 - mean_squared_error: 0.1071\n",
            "Epoch 00033: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.6184 - mean_absolute_error: 0.2612 - mean_squared_error: 0.1069 - val_loss: 1.5232 - val_mean_absolute_error: 0.1673 - val_mean_squared_error: 0.0289\n",
            "Epoch 34/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.5851 - mean_absolute_error: 0.2624 - mean_squared_error: 0.1075\n",
            "Epoch 00034: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.5850 - mean_absolute_error: 0.2624 - mean_squared_error: 0.1075 - val_loss: 1.4699 - val_mean_absolute_error: 0.0878 - val_mean_squared_error: 0.0092\n",
            "Epoch 35/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.5511 - mean_absolute_error: 0.2611 - mean_squared_error: 0.1069\n",
            "Epoch 00035: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.5511 - mean_absolute_error: 0.2613 - mean_squared_error: 0.1070 - val_loss: 1.4344 - val_mean_absolute_error: 0.0757 - val_mean_squared_error: 0.0069\n",
            "Epoch 36/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.5139 - mean_absolute_error: 0.2560 - mean_squared_error: 0.1028\n",
            "Epoch 00036: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.5141 - mean_absolute_error: 0.2563 - mean_squared_error: 0.1030 - val_loss: 1.3956 - val_mean_absolute_error: 0.0274 - val_mean_squared_error: 0.0010\n",
            "Epoch 37/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.4820 - mean_absolute_error: 0.2559 - mean_squared_error: 0.1032\n",
            "Epoch 00037: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.4818 - mean_absolute_error: 0.2558 - mean_squared_error: 0.1032 - val_loss: 1.4260 - val_mean_absolute_error: 0.2499 - val_mean_squared_error: 0.0631\n",
            "Epoch 38/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.4476 - mean_absolute_error: 0.2514 - mean_squared_error: 0.1001\n",
            "Epoch 00038: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.4475 - mean_absolute_error: 0.2513 - mean_squared_error: 0.1000 - val_loss: 1.3846 - val_mean_absolute_error: 0.2269 - val_mean_squared_error: 0.0524\n",
            "Epoch 39/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.4166 - mean_absolute_error: 0.2509 - mean_squared_error: 0.0994\n",
            "Epoch 00039: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.4164 - mean_absolute_error: 0.2507 - mean_squared_error: 0.0993 - val_loss: 1.3122 - val_mean_absolute_error: 0.0989 - val_mean_squared_error: 0.0102\n",
            "Epoch 40/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.3897 - mean_absolute_error: 0.2556 - mean_squared_error: 0.1023\n",
            "Epoch 00040: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.3896 - mean_absolute_error: 0.2555 - mean_squared_error: 0.1022 - val_loss: 1.3105 - val_mean_absolute_error: 0.1931 - val_mean_squared_error: 0.0377\n",
            "Epoch 41/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.3549 - mean_absolute_error: 0.2498 - mean_squared_error: 0.0966\n",
            "Epoch 00041: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.3551 - mean_absolute_error: 0.2500 - mean_squared_error: 0.0969 - val_loss: 1.3300 - val_mean_absolute_error: 0.2923 - val_mean_squared_error: 0.0863\n",
            "Epoch 42/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.3256 - mean_absolute_error: 0.2468 - mean_squared_error: 0.0961\n",
            "Epoch 00042: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.3256 - mean_absolute_error: 0.2468 - mean_squared_error: 0.0962 - val_loss: 1.2651 - val_mean_absolute_error: 0.2221 - val_mean_squared_error: 0.0500\n",
            "Epoch 43/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.2991 - mean_absolute_error: 0.2493 - mean_squared_error: 0.0979\n",
            "Epoch 00043: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.2991 - mean_absolute_error: 0.2492 - mean_squared_error: 0.0979 - val_loss: 1.2512 - val_mean_absolute_error: 0.2514 - val_mean_squared_error: 0.0639\n",
            "Epoch 44/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.2690 - mean_absolute_error: 0.2466 - mean_squared_error: 0.0950\n",
            "Epoch 00044: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.2689 - mean_absolute_error: 0.2466 - mean_squared_error: 0.0950 - val_loss: 1.2111 - val_mean_absolute_error: 0.2231 - val_mean_squared_error: 0.0506\n",
            "Epoch 45/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.2411 - mean_absolute_error: 0.2435 - mean_squared_error: 0.0937\n",
            "Epoch 00045: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.2412 - mean_absolute_error: 0.2437 - mean_squared_error: 0.0939 - val_loss: 1.1473 - val_mean_absolute_error: 0.1142 - val_mean_squared_error: 0.0133\n",
            "Epoch 46/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.2096 - mean_absolute_error: 0.2362 - mean_squared_error: 0.0887\n",
            "Epoch 00046: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.2097 - mean_absolute_error: 0.2365 - mean_squared_error: 0.0888 - val_loss: 1.1133 - val_mean_absolute_error: 0.0693 - val_mean_squared_error: 0.0056\n",
            "Epoch 47/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.1824 - mean_absolute_error: 0.2355 - mean_squared_error: 0.0873\n",
            "Epoch 00047: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.1824 - mean_absolute_error: 0.2354 - mean_squared_error: 0.0874 - val_loss: 1.0843 - val_mean_absolute_error: 0.0420 - val_mean_squared_error: 0.0021\n",
            "Epoch 48/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.1580 - mean_absolute_error: 0.2371 - mean_squared_error: 0.0884\n",
            "Epoch 00048: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.1580 - mean_absolute_error: 0.2371 - mean_squared_error: 0.0884 - val_loss: 1.0587 - val_mean_absolute_error: 0.0369 - val_mean_squared_error: 0.0019\n",
            "Epoch 49/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.1309 - mean_absolute_error: 0.2351 - mean_squared_error: 0.0863\n",
            "Epoch 00049: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.1308 - mean_absolute_error: 0.2350 - mean_squared_error: 0.0862 - val_loss: 1.0391 - val_mean_absolute_error: 0.0793 - val_mean_squared_error: 0.0070\n",
            "Epoch 50/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.1052 - mean_absolute_error: 0.2329 - mean_squared_error: 0.0852\n",
            "Epoch 00050: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.1050 - mean_absolute_error: 0.2325 - mean_squared_error: 0.0850 - val_loss: 1.0155 - val_mean_absolute_error: 0.0842 - val_mean_squared_error: 0.0078\n",
            "Epoch 51/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.0807 - mean_absolute_error: 0.2314 - mean_squared_error: 0.0848\n",
            "Epoch 00051: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.0807 - mean_absolute_error: 0.2315 - mean_squared_error: 0.0848 - val_loss: 0.9901 - val_mean_absolute_error: 0.0725 - val_mean_squared_error: 0.0059\n",
            "Epoch 52/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.0580 - mean_absolute_error: 0.2335 - mean_squared_error: 0.0850\n",
            "Epoch 00052: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.0580 - mean_absolute_error: 0.2335 - mean_squared_error: 0.0850 - val_loss: 0.9872 - val_mean_absolute_error: 0.1583 - val_mean_squared_error: 0.0254\n",
            "Epoch 53/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.0353 - mean_absolute_error: 0.2308 - mean_squared_error: 0.0845\n",
            "Epoch 00053: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.0352 - mean_absolute_error: 0.2306 - mean_squared_error: 0.0845 - val_loss: 0.9463 - val_mean_absolute_error: 0.0755 - val_mean_squared_error: 0.0067\n",
            "Epoch 54/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.0083 - mean_absolute_error: 0.2234 - mean_squared_error: 0.0796\n",
            "Epoch 00054: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 1.0081 - mean_absolute_error: 0.2233 - mean_squared_error: 0.0795 - val_loss: 0.9260 - val_mean_absolute_error: 0.0903 - val_mean_squared_error: 0.0083\n",
            "Epoch 55/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.9899 - mean_absolute_error: 0.2290 - mean_squared_error: 0.0828\n",
            "Epoch 00055: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.9897 - mean_absolute_error: 0.2288 - mean_squared_error: 0.0827 - val_loss: 0.8991 - val_mean_absolute_error: 0.0485 - val_mean_squared_error: 0.0027\n",
            "Epoch 56/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.9647 - mean_absolute_error: 0.2236 - mean_squared_error: 0.0787\n",
            "Epoch 00056: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.9648 - mean_absolute_error: 0.2237 - mean_squared_error: 0.0789 - val_loss: 0.8764 - val_mean_absolute_error: 0.0283 - val_mean_squared_error: 9.8599e-04\n",
            "Epoch 57/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.9447 - mean_absolute_error: 0.2247 - mean_squared_error: 0.0794\n",
            "Epoch 00057: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.9447 - mean_absolute_error: 0.2248 - mean_squared_error: 0.0794 - val_loss: 0.8573 - val_mean_absolute_error: 0.0417 - val_mean_squared_error: 0.0021\n",
            "Epoch 58/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.9225 - mean_absolute_error: 0.2210 - mean_squared_error: 0.0770\n",
            "Epoch 00058: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.9224 - mean_absolute_error: 0.2208 - mean_squared_error: 0.0770 - val_loss: 0.8427 - val_mean_absolute_error: 0.0824 - val_mean_squared_error: 0.0071\n",
            "Epoch 59/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.9030 - mean_absolute_error: 0.2197 - mean_squared_error: 0.0769\n",
            "Epoch 00059: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.9029 - mean_absolute_error: 0.2196 - mean_squared_error: 0.0769 - val_loss: 0.8302 - val_mean_absolute_error: 0.1154 - val_mean_squared_error: 0.0137\n",
            "Epoch 60/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.8842 - mean_absolute_error: 0.2208 - mean_squared_error: 0.0771\n",
            "Epoch 00060: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.8842 - mean_absolute_error: 0.2208 - mean_squared_error: 0.0771 - val_loss: 0.7995 - val_mean_absolute_error: 0.0413 - val_mean_squared_error: 0.0020\n",
            "Epoch 61/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.8639 - mean_absolute_error: 0.2187 - mean_squared_error: 0.0758\n",
            "Epoch 00061: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.8639 - mean_absolute_error: 0.2188 - mean_squared_error: 0.0758 - val_loss: 0.7818 - val_mean_absolute_error: 0.0539 - val_mean_squared_error: 0.0031\n",
            "Epoch 62/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.8458 - mean_absolute_error: 0.2201 - mean_squared_error: 0.0762\n",
            "Epoch 00062: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 21s 3ms/sample - loss: 0.8458 - mean_absolute_error: 0.2202 - mean_squared_error: 0.0763 - val_loss: 0.7710 - val_mean_absolute_error: 0.1007 - val_mean_squared_error: 0.0106\n",
            "Epoch 63/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.8245 - mean_absolute_error: 0.2153 - mean_squared_error: 0.0729\n",
            "Epoch 00063: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.8245 - mean_absolute_error: 0.2155 - mean_squared_error: 0.0730 - val_loss: 0.7486 - val_mean_absolute_error: 0.0753 - val_mean_squared_error: 0.0060\n",
            "Epoch 64/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.8075 - mean_absolute_error: 0.2156 - mean_squared_error: 0.0737\n",
            "Epoch 00064: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.8076 - mean_absolute_error: 0.2158 - mean_squared_error: 0.0738 - val_loss: 0.7258 - val_mean_absolute_error: 0.0233 - val_mean_squared_error: 7.6562e-04\n",
            "Epoch 65/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.7895 - mean_absolute_error: 0.2158 - mean_squared_error: 0.0730\n",
            "Epoch 00065: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.7895 - mean_absolute_error: 0.2158 - mean_squared_error: 0.0730 - val_loss: 0.7088 - val_mean_absolute_error: 0.0256 - val_mean_squared_error: 8.6710e-04\n",
            "Epoch 66/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.7712 - mean_absolute_error: 0.2135 - mean_squared_error: 0.0714\n",
            "Epoch 00066: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.7713 - mean_absolute_error: 0.2138 - mean_squared_error: 0.0716 - val_loss: 0.7040 - val_mean_absolute_error: 0.1114 - val_mean_squared_error: 0.0126\n",
            "Epoch 67/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.7546 - mean_absolute_error: 0.2130 - mean_squared_error: 0.0714\n",
            "Epoch 00067: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.7547 - mean_absolute_error: 0.2131 - mean_squared_error: 0.0714 - val_loss: 0.6788 - val_mean_absolute_error: 0.0587 - val_mean_squared_error: 0.0037\n",
            "Epoch 68/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.7385 - mean_absolute_error: 0.2122 - mean_squared_error: 0.0713\n",
            "Epoch 00068: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.7385 - mean_absolute_error: 0.2122 - mean_squared_error: 0.0713 - val_loss: 0.6667 - val_mean_absolute_error: 0.0854 - val_mean_squared_error: 0.0075\n",
            "Epoch 69/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.7213 - mean_absolute_error: 0.2103 - mean_squared_error: 0.0699\n",
            "Epoch 00069: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.7213 - mean_absolute_error: 0.2103 - mean_squared_error: 0.0699 - val_loss: 0.6698 - val_mean_absolute_error: 0.1614 - val_mean_squared_error: 0.0262\n",
            "Epoch 70/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.7050 - mean_absolute_error: 0.2077 - mean_squared_error: 0.0692\n",
            "Epoch 00070: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.7050 - mean_absolute_error: 0.2077 - mean_squared_error: 0.0692 - val_loss: 0.6347 - val_mean_absolute_error: 0.0800 - val_mean_squared_error: 0.0066\n",
            "Epoch 71/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.6884 - mean_absolute_error: 0.2065 - mean_squared_error: 0.0678\n",
            "Epoch 00071: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.6883 - mean_absolute_error: 0.2066 - mean_squared_error: 0.0678 - val_loss: 0.6151 - val_mean_absolute_error: 0.0427 - val_mean_squared_error: 0.0022\n",
            "Epoch 72/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.6728 - mean_absolute_error: 0.2061 - mean_squared_error: 0.0672\n",
            "Epoch 00072: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.6729 - mean_absolute_error: 0.2064 - mean_squared_error: 0.0674 - val_loss: 0.6457 - val_mean_absolute_error: 0.2172 - val_mean_squared_error: 0.0475\n",
            "Epoch 73/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.6583 - mean_absolute_error: 0.2064 - mean_squared_error: 0.0672\n",
            "Epoch 00073: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.6583 - mean_absolute_error: 0.2065 - mean_squared_error: 0.0673 - val_loss: 0.5880 - val_mean_absolute_error: 0.0632 - val_mean_squared_error: 0.0042\n",
            "Epoch 74/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.6445 - mean_absolute_error: 0.2053 - mean_squared_error: 0.0677\n",
            "Epoch 00074: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.6445 - mean_absolute_error: 0.2053 - mean_squared_error: 0.0677 - val_loss: 0.5744 - val_mean_absolute_error: 0.0648 - val_mean_squared_error: 0.0045\n",
            "Epoch 75/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.6287 - mean_absolute_error: 0.2036 - mean_squared_error: 0.0654\n",
            "Epoch 00075: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.6286 - mean_absolute_error: 0.2035 - mean_squared_error: 0.0653 - val_loss: 0.5678 - val_mean_absolute_error: 0.1045 - val_mean_squared_error: 0.0112\n",
            "Epoch 76/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.6141 - mean_absolute_error: 0.2007 - mean_squared_error: 0.0641\n",
            "Epoch 00076: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.6141 - mean_absolute_error: 0.2007 - mean_squared_error: 0.0641 - val_loss: 0.5453 - val_mean_absolute_error: 0.0413 - val_mean_squared_error: 0.0019\n",
            "Epoch 77/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.6006 - mean_absolute_error: 0.2006 - mean_squared_error: 0.0637\n",
            "Epoch 00077: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.6007 - mean_absolute_error: 0.2007 - mean_squared_error: 0.0638 - val_loss: 0.5343 - val_mean_absolute_error: 0.0601 - val_mean_squared_error: 0.0039\n",
            "Epoch 78/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.5892 - mean_absolute_error: 0.2023 - mean_squared_error: 0.0651\n",
            "Epoch 00078: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.5891 - mean_absolute_error: 0.2022 - mean_squared_error: 0.0651 - val_loss: 0.5186 - val_mean_absolute_error: 0.0248 - val_mean_squared_error: 8.4003e-04\n",
            "Epoch 79/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.5737 - mean_absolute_error: 0.1971 - mean_squared_error: 0.0621\n",
            "Epoch 00079: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.5736 - mean_absolute_error: 0.1971 - mean_squared_error: 0.0621 - val_loss: 0.5169 - val_mean_absolute_error: 0.1066 - val_mean_squared_error: 0.0115\n",
            "Epoch 80/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.5638 - mean_absolute_error: 0.2017 - mean_squared_error: 0.0644\n",
            "Epoch 00080: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.5639 - mean_absolute_error: 0.2018 - mean_squared_error: 0.0645 - val_loss: 0.5154 - val_mean_absolute_error: 0.1478 - val_mean_squared_error: 0.0221\n",
            "Epoch 81/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.5487 - mean_absolute_error: 0.1958 - mean_squared_error: 0.0612\n",
            "Epoch 00081: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.5488 - mean_absolute_error: 0.1957 - mean_squared_error: 0.0612 - val_loss: 0.4862 - val_mean_absolute_error: 0.0644 - val_mean_squared_error: 0.0045\n",
            "Epoch 82/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.5374 - mean_absolute_error: 0.1955 - mean_squared_error: 0.0613\n",
            "Epoch 00082: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.5373 - mean_absolute_error: 0.1954 - mean_squared_error: 0.0613 - val_loss: 0.4855 - val_mean_absolute_error: 0.1224 - val_mean_squared_error: 0.0152\n",
            "Epoch 83/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.5259 - mean_absolute_error: 0.1969 - mean_squared_error: 0.0612\n",
            "Epoch 00083: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.5258 - mean_absolute_error: 0.1968 - mean_squared_error: 0.0612 - val_loss: 0.4607 - val_mean_absolute_error: 0.0394 - val_mean_squared_error: 0.0017\n",
            "Epoch 84/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.5139 - mean_absolute_error: 0.1948 - mean_squared_error: 0.0605\n",
            "Epoch 00084: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 2ms/sample - loss: 0.5139 - mean_absolute_error: 0.1948 - mean_squared_error: 0.0605 - val_loss: 0.4543 - val_mean_absolute_error: 0.0786 - val_mean_squared_error: 0.0064\n",
            "Epoch 85/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.5025 - mean_absolute_error: 0.1939 - mean_squared_error: 0.0599\n",
            "Epoch 00085: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.5025 - mean_absolute_error: 0.1939 - mean_squared_error: 0.0600 - val_loss: 0.4400 - val_mean_absolute_error: 0.0521 - val_mean_squared_error: 0.0029\n",
            "Epoch 86/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4917 - mean_absolute_error: 0.1934 - mean_squared_error: 0.0598\n",
            "Epoch 00086: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.4917 - mean_absolute_error: 0.1934 - mean_squared_error: 0.0598 - val_loss: 0.4346 - val_mean_absolute_error: 0.0889 - val_mean_squared_error: 0.0081\n",
            "Epoch 87/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4797 - mean_absolute_error: 0.1917 - mean_squared_error: 0.0584\n",
            "Epoch 00087: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.4797 - mean_absolute_error: 0.1918 - mean_squared_error: 0.0584 - val_loss: 0.4270 - val_mean_absolute_error: 0.1031 - val_mean_squared_error: 0.0109\n",
            "Epoch 88/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4711 - mean_absolute_error: 0.1937 - mean_squared_error: 0.0600\n",
            "Epoch 00088: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.4711 - mean_absolute_error: 0.1936 - mean_squared_error: 0.0600 - val_loss: 0.4155 - val_mean_absolute_error: 0.0958 - val_mean_squared_error: 0.0094\n",
            "Epoch 89/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4600 - mean_absolute_error: 0.1917 - mean_squared_error: 0.0588\n",
            "Epoch 00089: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.4600 - mean_absolute_error: 0.1917 - mean_squared_error: 0.0588 - val_loss: 0.3965 - val_mean_absolute_error: 0.0132 - val_mean_squared_error: 3.1491e-04\n",
            "Epoch 90/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4509 - mean_absolute_error: 0.1931 - mean_squared_error: 0.0594\n",
            "Epoch 00090: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.4509 - mean_absolute_error: 0.1932 - mean_squared_error: 0.0595 - val_loss: 0.3967 - val_mean_absolute_error: 0.0998 - val_mean_squared_error: 0.0101\n",
            "Epoch 91/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4385 - mean_absolute_error: 0.1882 - mean_squared_error: 0.0566\n",
            "Epoch 00091: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.4385 - mean_absolute_error: 0.1881 - mean_squared_error: 0.0566 - val_loss: 0.3774 - val_mean_absolute_error: 0.0113 - val_mean_squared_error: 2.3082e-04\n",
            "Epoch 92/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4293 - mean_absolute_error: 0.1885 - mean_squared_error: 0.0566\n",
            "Epoch 00092: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.4293 - mean_absolute_error: 0.1886 - mean_squared_error: 0.0567 - val_loss: 0.3695 - val_mean_absolute_error: 0.0302 - val_mean_squared_error: 0.0015\n",
            "Epoch 93/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4185 - mean_absolute_error: 0.1851 - mean_squared_error: 0.0548\n",
            "Epoch 00093: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.4185 - mean_absolute_error: 0.1851 - mean_squared_error: 0.0548 - val_loss: 0.3776 - val_mean_absolute_error: 0.1343 - val_mean_squared_error: 0.0183\n",
            "Epoch 94/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4113 - mean_absolute_error: 0.1880 - mean_squared_error: 0.0562\n",
            "Epoch 00094: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.4113 - mean_absolute_error: 0.1880 - mean_squared_error: 0.0562 - val_loss: 0.3552 - val_mean_absolute_error: 0.0648 - val_mean_squared_error: 0.0043\n",
            "Epoch 95/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4025 - mean_absolute_error: 0.1877 - mean_squared_error: 0.0559\n",
            "Epoch 00095: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.4025 - mean_absolute_error: 0.1876 - mean_squared_error: 0.0558 - val_loss: 0.3441 - val_mean_absolute_error: 0.0396 - val_mean_squared_error: 0.0017\n",
            "Epoch 96/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.3936 - mean_absolute_error: 0.1859 - mean_squared_error: 0.0553\n",
            "Epoch 00096: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.3936 - mean_absolute_error: 0.1859 - mean_squared_error: 0.0553 - val_loss: 0.3345 - val_mean_absolute_error: 0.0142 - val_mean_squared_error: 3.7588e-04\n",
            "Epoch 97/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.3834 - mean_absolute_error: 0.1820 - mean_squared_error: 0.0532\n",
            "Epoch 00097: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.3834 - mean_absolute_error: 0.1822 - mean_squared_error: 0.0533 - val_loss: 0.3305 - val_mean_absolute_error: 0.0647 - val_mean_squared_error: 0.0044\n",
            "Epoch 98/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.3761 - mean_absolute_error: 0.1835 - mean_squared_error: 0.0539\n",
            "Epoch 00098: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.3762 - mean_absolute_error: 0.1837 - mean_squared_error: 0.0540 - val_loss: 0.3294 - val_mean_absolute_error: 0.1049 - val_mean_squared_error: 0.0112\n",
            "Epoch 99/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.3669 - mean_absolute_error: 0.1815 - mean_squared_error: 0.0526\n",
            "Epoch 00099: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.3670 - mean_absolute_error: 0.1815 - mean_squared_error: 0.0527 - val_loss: 0.3109 - val_mean_absolute_error: 0.0202 - val_mean_squared_error: 5.3389e-04\n",
            "Epoch 100/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.3612 - mean_absolute_error: 0.1845 - mean_squared_error: 0.0546\n",
            "Epoch 00100: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 20s 3ms/sample - loss: 0.3613 - mean_absolute_error: 0.1847 - mean_squared_error: 0.0547 - val_loss: 0.3054 - val_mean_absolute_error: 0.0481 - val_mean_squared_error: 0.0025\n",
            "Train on 8032 samples, validate on 2016 samples\n",
            "Epoch 1/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 1.5392 - mean_absolute_error: 1.0185 - mean_squared_error: 1.5392\n",
            "Epoch 00001: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 4s 520us/sample - loss: 1.5373 - mean_absolute_error: 1.0177 - mean_squared_error: 1.5373 - val_loss: 0.4132 - val_mean_absolute_error: 0.6398 - val_mean_squared_error: 0.4132\n",
            "Epoch 2/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.7042 - mean_absolute_error: 0.6639 - mean_squared_error: 0.7042\n",
            "Epoch 00002: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 201us/sample - loss: 0.7027 - mean_absolute_error: 0.6631 - mean_squared_error: 0.7027 - val_loss: 0.0607 - val_mean_absolute_error: 0.2444 - val_mean_squared_error: 0.0607\n",
            "Epoch 3/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.5041 - mean_absolute_error: 0.5594 - mean_squared_error: 0.5041\n",
            "Epoch 00003: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 199us/sample - loss: 0.5010 - mean_absolute_error: 0.5575 - mean_squared_error: 0.5010 - val_loss: 0.0205 - val_mean_absolute_error: 0.1410 - val_mean_squared_error: 0.0205\n",
            "Epoch 4/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 0.3630 - mean_absolute_error: 0.4767 - mean_squared_error: 0.3630\n",
            "Epoch 00004: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 208us/sample - loss: 0.3602 - mean_absolute_error: 0.4747 - mean_squared_error: 0.3602 - val_loss: 0.0273 - val_mean_absolute_error: 0.1633 - val_mean_squared_error: 0.0273\n",
            "Epoch 5/100\n",
            "7744/8032 [===========================>..] - ETA: 0s - loss: 0.2969 - mean_absolute_error: 0.4307 - mean_squared_error: 0.2969\n",
            "Epoch 00005: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 199us/sample - loss: 0.2950 - mean_absolute_error: 0.4296 - mean_squared_error: 0.2950 - val_loss: 0.0386 - val_mean_absolute_error: 0.1946 - val_mean_squared_error: 0.0386\n",
            "Epoch 6/100\n",
            "7904/8032 [============================>.] - ETA: 0s - loss: 0.2408 - mean_absolute_error: 0.3909 - mean_squared_error: 0.2408\n",
            "Epoch 00006: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 197us/sample - loss: 0.2414 - mean_absolute_error: 0.3913 - mean_squared_error: 0.2414 - val_loss: 0.0413 - val_mean_absolute_error: 0.2013 - val_mean_squared_error: 0.0413\n",
            "Epoch 7/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.2043 - mean_absolute_error: 0.3602 - mean_squared_error: 0.2043\n",
            "Epoch 00007: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 201us/sample - loss: 0.2044 - mean_absolute_error: 0.3604 - mean_squared_error: 0.2044 - val_loss: 0.0473 - val_mean_absolute_error: 0.2156 - val_mean_squared_error: 0.0473\n",
            "Epoch 8/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.1824 - mean_absolute_error: 0.3404 - mean_squared_error: 0.1824\n",
            "Epoch 00008: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 198us/sample - loss: 0.1819 - mean_absolute_error: 0.3397 - mean_squared_error: 0.1819 - val_loss: 0.0496 - val_mean_absolute_error: 0.2208 - val_mean_squared_error: 0.0496\n",
            "Epoch 9/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.1645 - mean_absolute_error: 0.3246 - mean_squared_error: 0.1645\n",
            "Epoch 00009: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 203us/sample - loss: 0.1643 - mean_absolute_error: 0.3244 - mean_squared_error: 0.1643 - val_loss: 0.0497 - val_mean_absolute_error: 0.2210 - val_mean_squared_error: 0.0497\n",
            "Epoch 10/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.1564 - mean_absolute_error: 0.3162 - mean_squared_error: 0.1564\n",
            "Epoch 00010: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 199us/sample - loss: 0.1565 - mean_absolute_error: 0.3164 - mean_squared_error: 0.1565 - val_loss: 0.0515 - val_mean_absolute_error: 0.2249 - val_mean_squared_error: 0.0515\n",
            "Epoch 11/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.1449 - mean_absolute_error: 0.3056 - mean_squared_error: 0.1449\n",
            "Epoch 00011: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 198us/sample - loss: 0.1450 - mean_absolute_error: 0.3054 - mean_squared_error: 0.1450 - val_loss: 0.0504 - val_mean_absolute_error: 0.2225 - val_mean_squared_error: 0.0504\n",
            "Epoch 12/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.1381 - mean_absolute_error: 0.2971 - mean_squared_error: 0.1381\n",
            "Epoch 00012: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 196us/sample - loss: 0.1378 - mean_absolute_error: 0.2966 - mean_squared_error: 0.1378 - val_loss: 0.0543 - val_mean_absolute_error: 0.2310 - val_mean_squared_error: 0.0543\n",
            "Epoch 13/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.1318 - mean_absolute_error: 0.2903 - mean_squared_error: 0.1318\n",
            "Epoch 00013: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 198us/sample - loss: 0.1317 - mean_absolute_error: 0.2902 - mean_squared_error: 0.1317 - val_loss: 0.0482 - val_mean_absolute_error: 0.2175 - val_mean_squared_error: 0.0482\n",
            "Epoch 14/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 0.1266 - mean_absolute_error: 0.2847 - mean_squared_error: 0.1266\n",
            "Epoch 00014: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 199us/sample - loss: 0.1265 - mean_absolute_error: 0.2846 - mean_squared_error: 0.1265 - val_loss: 0.0471 - val_mean_absolute_error: 0.2148 - val_mean_squared_error: 0.0471\n",
            "Epoch 15/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 0.1230 - mean_absolute_error: 0.2795 - mean_squared_error: 0.1230\n",
            "Epoch 00015: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 197us/sample - loss: 0.1232 - mean_absolute_error: 0.2796 - mean_squared_error: 0.1232 - val_loss: 0.0498 - val_mean_absolute_error: 0.2209 - val_mean_squared_error: 0.0498\n",
            "Epoch 16/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.1166 - mean_absolute_error: 0.2727 - mean_squared_error: 0.1166\n",
            "Epoch 00016: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 205us/sample - loss: 0.1164 - mean_absolute_error: 0.2726 - mean_squared_error: 0.1164 - val_loss: 0.0440 - val_mean_absolute_error: 0.2076 - val_mean_squared_error: 0.0440\n",
            "Epoch 17/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.1152 - mean_absolute_error: 0.2709 - mean_squared_error: 0.1152\n",
            "Epoch 00017: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 214us/sample - loss: 0.1155 - mean_absolute_error: 0.2713 - mean_squared_error: 0.1155 - val_loss: 0.0391 - val_mean_absolute_error: 0.1953 - val_mean_squared_error: 0.0391\n",
            "Epoch 18/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.1122 - mean_absolute_error: 0.2666 - mean_squared_error: 0.1122\n",
            "Epoch 00018: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 203us/sample - loss: 0.1121 - mean_absolute_error: 0.2666 - mean_squared_error: 0.1121 - val_loss: 0.0404 - val_mean_absolute_error: 0.1985 - val_mean_squared_error: 0.0404\n",
            "Epoch 19/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1079 - mean_absolute_error: 0.2614 - mean_squared_error: 0.1079\n",
            "Epoch 00019: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 202us/sample - loss: 0.1079 - mean_absolute_error: 0.2614 - mean_squared_error: 0.1079 - val_loss: 0.0331 - val_mean_absolute_error: 0.1796 - val_mean_squared_error: 0.0331\n",
            "Epoch 20/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 0.1031 - mean_absolute_error: 0.2560 - mean_squared_error: 0.1031\n",
            "Epoch 00020: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 198us/sample - loss: 0.1028 - mean_absolute_error: 0.2555 - mean_squared_error: 0.1028 - val_loss: 0.0283 - val_mean_absolute_error: 0.1656 - val_mean_squared_error: 0.0283\n",
            "Epoch 21/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 0.1009 - mean_absolute_error: 0.2515 - mean_squared_error: 0.1009\n",
            "Epoch 00021: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 200us/sample - loss: 0.1008 - mean_absolute_error: 0.2514 - mean_squared_error: 0.1008 - val_loss: 0.0315 - val_mean_absolute_error: 0.1751 - val_mean_squared_error: 0.0315\n",
            "Epoch 22/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.0982 - mean_absolute_error: 0.2485 - mean_squared_error: 0.0982\n",
            "Epoch 00022: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 198us/sample - loss: 0.0980 - mean_absolute_error: 0.2482 - mean_squared_error: 0.0980 - val_loss: 0.0235 - val_mean_absolute_error: 0.1505 - val_mean_squared_error: 0.0235\n",
            "Epoch 23/100\n",
            "7744/8032 [===========================>..] - ETA: 0s - loss: 0.0986 - mean_absolute_error: 0.2489 - mean_squared_error: 0.0986\n",
            "Epoch 00023: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 208us/sample - loss: 0.0983 - mean_absolute_error: 0.2486 - mean_squared_error: 0.0983 - val_loss: 0.0262 - val_mean_absolute_error: 0.1592 - val_mean_squared_error: 0.0262\n",
            "Train on 8032 samples, validate on 2016 samples\n",
            "Epoch 1/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 1.9789 - mean_absolute_error: 0.9321 - mean_squared_error: 1.0161\n",
            "Epoch 00001: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 289us/sample - loss: 1.9665 - mean_absolute_error: 0.9249 - mean_squared_error: 1.0043 - val_loss: 1.3896 - val_mean_absolute_error: 0.6609 - val_mean_squared_error: 0.4440\n",
            "Epoch 2/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 1.3363 - mean_absolute_error: 0.5306 - mean_squared_error: 0.4079\n",
            "Epoch 00002: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 224us/sample - loss: 1.3346 - mean_absolute_error: 0.5297 - mean_squared_error: 0.4066 - val_loss: 1.0053 - val_mean_absolute_error: 0.3031 - val_mean_squared_error: 0.0958\n",
            "Epoch 3/100\n",
            "7840/8032 [============================>.] - ETA: 0s - loss: 1.1887 - mean_absolute_error: 0.4465 - mean_squared_error: 0.2982\n",
            "Epoch 00003: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 217us/sample - loss: 1.1885 - mean_absolute_error: 0.4467 - mean_squared_error: 0.2984 - val_loss: 0.9418 - val_mean_absolute_error: 0.2611 - val_mean_squared_error: 0.0714\n",
            "Epoch 4/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 1.1236 - mean_absolute_error: 0.4264 - mean_squared_error: 0.2724\n",
            "Epoch 00004: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 214us/sample - loss: 1.1232 - mean_absolute_error: 0.4263 - mean_squared_error: 0.2723 - val_loss: 0.8859 - val_mean_absolute_error: 0.2270 - val_mean_squared_error: 0.0546\n",
            "Epoch 5/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 1.0526 - mean_absolute_error: 0.3997 - mean_squared_error: 0.2399\n",
            "Epoch 00005: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 213us/sample - loss: 1.0519 - mean_absolute_error: 0.3994 - mean_squared_error: 0.2398 - val_loss: 0.8372 - val_mean_absolute_error: 0.2036 - val_mean_squared_error: 0.0444\n",
            "Epoch 6/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.9930 - mean_absolute_error: 0.3804 - mean_squared_error: 0.2189\n",
            "Epoch 00006: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 215us/sample - loss: 0.9926 - mean_absolute_error: 0.3801 - mean_squared_error: 0.2186 - val_loss: 0.7963 - val_mean_absolute_error: 0.1950 - val_mean_squared_error: 0.0410\n",
            "Epoch 7/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.9469 - mean_absolute_error: 0.3704 - mean_squared_error: 0.2091\n",
            "Epoch 00007: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 219us/sample - loss: 0.9463 - mean_absolute_error: 0.3705 - mean_squared_error: 0.2090 - val_loss: 0.7519 - val_mean_absolute_error: 0.1729 - val_mean_squared_error: 0.0325\n",
            "Epoch 8/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.8992 - mean_absolute_error: 0.3593 - mean_squared_error: 0.1967\n",
            "Epoch 00008: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 216us/sample - loss: 0.8987 - mean_absolute_error: 0.3593 - mean_squared_error: 0.1966 - val_loss: 0.7159 - val_mean_absolute_error: 0.1682 - val_mean_squared_error: 0.0308\n",
            "Epoch 9/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.8496 - mean_absolute_error: 0.3447 - mean_squared_error: 0.1809\n",
            "Epoch 00009: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 213us/sample - loss: 0.8493 - mean_absolute_error: 0.3443 - mean_squared_error: 0.1805 - val_loss: 0.6816 - val_mean_absolute_error: 0.1629 - val_mean_squared_error: 0.0290\n",
            "Epoch 10/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.8108 - mean_absolute_error: 0.3363 - mean_squared_error: 0.1737\n",
            "Epoch 00010: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 211us/sample - loss: 0.8106 - mean_absolute_error: 0.3361 - mean_squared_error: 0.1736 - val_loss: 0.6532 - val_mean_absolute_error: 0.1703 - val_mean_squared_error: 0.0316\n",
            "Epoch 11/100\n",
            "7744/8032 [===========================>..] - ETA: 0s - loss: 0.7740 - mean_absolute_error: 0.3286 - mean_squared_error: 0.1665\n",
            "Epoch 00011: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 220us/sample - loss: 0.7726 - mean_absolute_error: 0.3276 - mean_squared_error: 0.1656 - val_loss: 0.6234 - val_mean_absolute_error: 0.1685 - val_mean_squared_error: 0.0308\n",
            "Epoch 12/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.7428 - mean_absolute_error: 0.3262 - mean_squared_error: 0.1636\n",
            "Epoch 00012: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 217us/sample - loss: 0.7421 - mean_absolute_error: 0.3257 - mean_squared_error: 0.1632 - val_loss: 0.5909 - val_mean_absolute_error: 0.1531 - val_mean_squared_error: 0.0256\n",
            "Epoch 13/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.7071 - mean_absolute_error: 0.3165 - mean_squared_error: 0.1543\n",
            "Epoch 00013: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 223us/sample - loss: 0.7070 - mean_absolute_error: 0.3167 - mean_squared_error: 0.1546 - val_loss: 0.5647 - val_mean_absolute_error: 0.1515 - val_mean_squared_error: 0.0251\n",
            "Epoch 14/100\n",
            "7840/8032 [============================>.] - ETA: 0s - loss: 0.6763 - mean_absolute_error: 0.3082 - mean_squared_error: 0.1484\n",
            "Epoch 00014: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 223us/sample - loss: 0.6763 - mean_absolute_error: 0.3084 - mean_squared_error: 0.1487 - val_loss: 0.5392 - val_mean_absolute_error: 0.1468 - val_mean_squared_error: 0.0235\n",
            "Epoch 15/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.6534 - mean_absolute_error: 0.3107 - mean_squared_error: 0.1490\n",
            "Epoch 00015: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 220us/sample - loss: 0.6531 - mean_absolute_error: 0.3104 - mean_squared_error: 0.1488 - val_loss: 0.5157 - val_mean_absolute_error: 0.1437 - val_mean_squared_error: 0.0226\n",
            "Epoch 16/100\n",
            "7840/8032 [============================>.] - ETA: 0s - loss: 0.6247 - mean_absolute_error: 0.3035 - mean_squared_error: 0.1419\n",
            "Epoch 00016: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 218us/sample - loss: 0.6242 - mean_absolute_error: 0.3033 - mean_squared_error: 0.1417 - val_loss: 0.4963 - val_mean_absolute_error: 0.1493 - val_mean_squared_error: 0.0242\n",
            "Epoch 17/100\n",
            "7840/8032 [============================>.] - ETA: 0s - loss: 0.5962 - mean_absolute_error: 0.2934 - mean_squared_error: 0.1337\n",
            "Epoch 00017: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 212us/sample - loss: 0.5959 - mean_absolute_error: 0.2932 - mean_squared_error: 0.1336 - val_loss: 0.4701 - val_mean_absolute_error: 0.1253 - val_mean_squared_error: 0.0175\n",
            "Epoch 18/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.5790 - mean_absolute_error: 0.2945 - mean_squared_error: 0.1356\n",
            "Epoch 00018: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 214us/sample - loss: 0.5789 - mean_absolute_error: 0.2945 - mean_squared_error: 0.1356 - val_loss: 0.4527 - val_mean_absolute_error: 0.1290 - val_mean_squared_error: 0.0184\n",
            "Epoch 19/100\n",
            "7840/8032 [============================>.] - ETA: 0s - loss: 0.5546 - mean_absolute_error: 0.2868 - mean_squared_error: 0.1287\n",
            "Epoch 00019: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 215us/sample - loss: 0.5542 - mean_absolute_error: 0.2866 - mean_squared_error: 0.1285 - val_loss: 0.4366 - val_mean_absolute_error: 0.1328 - val_mean_squared_error: 0.0194\n",
            "Epoch 20/100\n",
            "7840/8032 [============================>.] - ETA: 0s - loss: 0.5371 - mean_absolute_error: 0.2869 - mean_squared_error: 0.1278\n",
            "Epoch 00020: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 211us/sample - loss: 0.5368 - mean_absolute_error: 0.2867 - mean_squared_error: 0.1277 - val_loss: 0.4193 - val_mean_absolute_error: 0.1282 - val_mean_squared_error: 0.0181\n",
            "Epoch 21/100\n",
            "7744/8032 [===========================>..] - ETA: 0s - loss: 0.5190 - mean_absolute_error: 0.2823 - mean_squared_error: 0.1250\n",
            "Epoch 00021: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 216us/sample - loss: 0.5188 - mean_absolute_error: 0.2825 - mean_squared_error: 0.1251 - val_loss: 0.4014 - val_mean_absolute_error: 0.1154 - val_mean_squared_error: 0.0149\n",
            "Epoch 22/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4982 - mean_absolute_error: 0.2767 - mean_squared_error: 0.1188\n",
            "Epoch 00022: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 225us/sample - loss: 0.4982 - mean_absolute_error: 0.2767 - mean_squared_error: 0.1188 - val_loss: 0.3857 - val_mean_absolute_error: 0.1085 - val_mean_squared_error: 0.0133\n",
            "Epoch 23/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.4846 - mean_absolute_error: 0.2765 - mean_squared_error: 0.1187\n",
            "Epoch 00023: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 214us/sample - loss: 0.4847 - mean_absolute_error: 0.2767 - mean_squared_error: 0.1189 - val_loss: 0.3752 - val_mean_absolute_error: 0.1199 - val_mean_squared_error: 0.0159\n",
            "Epoch 24/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.4743 - mean_absolute_error: 0.2780 - mean_squared_error: 0.1211\n",
            "Epoch 00024: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 215us/sample - loss: 0.4743 - mean_absolute_error: 0.2779 - mean_squared_error: 0.1211 - val_loss: 0.3607 - val_mean_absolute_error: 0.1103 - val_mean_squared_error: 0.0136\n",
            "Epoch 25/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.4588 - mean_absolute_error: 0.2734 - mean_squared_error: 0.1174\n",
            "Epoch 00025: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 215us/sample - loss: 0.4589 - mean_absolute_error: 0.2739 - mean_squared_error: 0.1176 - val_loss: 0.3516 - val_mean_absolute_error: 0.1204 - val_mean_squared_error: 0.0160\n",
            "Epoch 26/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.4436 - mean_absolute_error: 0.2693 - mean_squared_error: 0.1134\n",
            "Epoch 00026: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 220us/sample - loss: 0.4436 - mean_absolute_error: 0.2693 - mean_squared_error: 0.1134 - val_loss: 0.3411 - val_mean_absolute_error: 0.1220 - val_mean_squared_error: 0.0163\n",
            "Epoch 27/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.4321 - mean_absolute_error: 0.2690 - mean_squared_error: 0.1124\n",
            "Epoch 00027: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 215us/sample - loss: 0.4320 - mean_absolute_error: 0.2690 - mean_squared_error: 0.1124 - val_loss: 0.3293 - val_mean_absolute_error: 0.1158 - val_mean_squared_error: 0.0148\n",
            "Epoch 28/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.4199 - mean_absolute_error: 0.2640 - mean_squared_error: 0.1101\n",
            "Epoch 00028: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 219us/sample - loss: 0.4195 - mean_absolute_error: 0.2637 - mean_squared_error: 0.1099 - val_loss: 0.3193 - val_mean_absolute_error: 0.1143 - val_mean_squared_error: 0.0144\n",
            "Epoch 29/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.4117 - mean_absolute_error: 0.2650 - mean_squared_error: 0.1113\n",
            "Epoch 00029: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 223us/sample - loss: 0.4114 - mean_absolute_error: 0.2648 - mean_squared_error: 0.1111 - val_loss: 0.3099 - val_mean_absolute_error: 0.1127 - val_mean_squared_error: 0.0140\n",
            "Epoch 30/100\n",
            "7840/8032 [============================>.] - ETA: 0s - loss: 0.3976 - mean_absolute_error: 0.2597 - mean_squared_error: 0.1060\n",
            "Epoch 00030: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 219us/sample - loss: 0.3973 - mean_absolute_error: 0.2597 - mean_squared_error: 0.1058 - val_loss: 0.2978 - val_mean_absolute_error: 0.0966 - val_mean_squared_error: 0.0105\n",
            "Epoch 31/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 0.3897 - mean_absolute_error: 0.2614 - mean_squared_error: 0.1064\n",
            "Epoch 00031: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 215us/sample - loss: 0.3895 - mean_absolute_error: 0.2614 - mean_squared_error: 0.1064 - val_loss: 0.2922 - val_mean_absolute_error: 0.1091 - val_mean_squared_error: 0.0131\n",
            "Epoch 32/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.3810 - mean_absolute_error: 0.2609 - mean_squared_error: 0.1059\n",
            "Epoch 00032: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 220us/sample - loss: 0.3811 - mean_absolute_error: 0.2610 - mean_squared_error: 0.1059 - val_loss: 0.2845 - val_mean_absolute_error: 0.1095 - val_mean_squared_error: 0.0132\n",
            "Epoch 33/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.3700 - mean_absolute_error: 0.2553 - mean_squared_error: 0.1024\n",
            "Epoch 00033: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 215us/sample - loss: 0.3699 - mean_absolute_error: 0.2551 - mean_squared_error: 0.1023 - val_loss: 0.2753 - val_mean_absolute_error: 0.1010 - val_mean_squared_error: 0.0114\n",
            "Epoch 34/100\n",
            "7840/8032 [============================>.] - ETA: 0s - loss: 0.3611 - mean_absolute_error: 0.2536 - mean_squared_error: 0.1007\n",
            "Epoch 00034: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 227us/sample - loss: 0.3608 - mean_absolute_error: 0.2534 - mean_squared_error: 0.1005 - val_loss: 0.2689 - val_mean_absolute_error: 0.1047 - val_mean_squared_error: 0.0121\n",
            "Epoch 35/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.3549 - mean_absolute_error: 0.2550 - mean_squared_error: 0.1015\n",
            "Epoch 00035: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 222us/sample - loss: 0.3548 - mean_absolute_error: 0.2549 - mean_squared_error: 0.1014 - val_loss: 0.2613 - val_mean_absolute_error: 0.1005 - val_mean_squared_error: 0.0112\n",
            "Epoch 36/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.3432 - mean_absolute_error: 0.2458 - mean_squared_error: 0.0963\n",
            "Epoch 00036: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 222us/sample - loss: 0.3431 - mean_absolute_error: 0.2457 - mean_squared_error: 0.0962 - val_loss: 0.2559 - val_mean_absolute_error: 0.1054 - val_mean_squared_error: 0.0122\n",
            "Epoch 37/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.3395 - mean_absolute_error: 0.2518 - mean_squared_error: 0.0989\n",
            "Epoch 00037: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 221us/sample - loss: 0.3397 - mean_absolute_error: 0.2521 - mean_squared_error: 0.0991 - val_loss: 0.2496 - val_mean_absolute_error: 0.1046 - val_mean_squared_error: 0.0120\n",
            "Epoch 38/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.3318 - mean_absolute_error: 0.2479 - mean_squared_error: 0.0971\n",
            "Epoch 00038: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 216us/sample - loss: 0.3318 - mean_absolute_error: 0.2480 - mean_squared_error: 0.0971 - val_loss: 0.2448 - val_mean_absolute_error: 0.1090 - val_mean_squared_error: 0.0130\n",
            "Epoch 39/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.3250 - mean_absolute_error: 0.2473 - mean_squared_error: 0.0958\n",
            "Epoch 00039: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 221us/sample - loss: 0.3250 - mean_absolute_error: 0.2474 - mean_squared_error: 0.0959 - val_loss: 0.2379 - val_mean_absolute_error: 0.1024 - val_mean_squared_error: 0.0115\n",
            "Epoch 40/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.3183 - mean_absolute_error: 0.2452 - mean_squared_error: 0.0945\n",
            "Epoch 00040: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 217us/sample - loss: 0.3182 - mean_absolute_error: 0.2452 - mean_squared_error: 0.0945 - val_loss: 0.2325 - val_mean_absolute_error: 0.1018 - val_mean_squared_error: 0.0114\n",
            "Epoch 41/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 0.3125 - mean_absolute_error: 0.2437 - mean_squared_error: 0.0938\n",
            "Epoch 00041: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 215us/sample - loss: 0.3128 - mean_absolute_error: 0.2440 - mean_squared_error: 0.0942 - val_loss: 0.2272 - val_mean_absolute_error: 0.1004 - val_mean_squared_error: 0.0111\n",
            "Epoch 42/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.3079 - mean_absolute_error: 0.2435 - mean_squared_error: 0.0942\n",
            "Epoch 00042: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 211us/sample - loss: 0.3077 - mean_absolute_error: 0.2433 - mean_squared_error: 0.0940 - val_loss: 0.2211 - val_mean_absolute_error: 0.0943 - val_mean_squared_error: 0.0099\n",
            "Epoch 43/100\n",
            "7840/8032 [============================>.] - ETA: 0s - loss: 0.3024 - mean_absolute_error: 0.2428 - mean_squared_error: 0.0934\n",
            "Epoch 00043: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 215us/sample - loss: 0.3022 - mean_absolute_error: 0.2427 - mean_squared_error: 0.0933 - val_loss: 0.2173 - val_mean_absolute_error: 0.0988 - val_mean_squared_error: 0.0107\n",
            "Epoch 44/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.2961 - mean_absolute_error: 0.2412 - mean_squared_error: 0.0916\n",
            "Epoch 00044: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 220us/sample - loss: 0.2959 - mean_absolute_error: 0.2411 - mean_squared_error: 0.0915 - val_loss: 0.2144 - val_mean_absolute_error: 0.1061 - val_mean_squared_error: 0.0122\n",
            "Epoch 45/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.2905 - mean_absolute_error: 0.2394 - mean_squared_error: 0.0905\n",
            "Epoch 00045: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 226us/sample - loss: 0.2905 - mean_absolute_error: 0.2394 - mean_squared_error: 0.0904 - val_loss: 0.2087 - val_mean_absolute_error: 0.0989 - val_mean_squared_error: 0.0107\n",
            "Epoch 46/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.2838 - mean_absolute_error: 0.2362 - mean_squared_error: 0.0878\n",
            "Epoch 00046: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 238us/sample - loss: 0.2836 - mean_absolute_error: 0.2359 - mean_squared_error: 0.0877 - val_loss: 0.2043 - val_mean_absolute_error: 0.0976 - val_mean_squared_error: 0.0104\n",
            "Epoch 47/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.2817 - mean_absolute_error: 0.2377 - mean_squared_error: 0.0897\n",
            "Epoch 00047: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 227us/sample - loss: 0.2817 - mean_absolute_error: 0.2378 - mean_squared_error: 0.0898 - val_loss: 0.1999 - val_mean_absolute_error: 0.0951 - val_mean_squared_error: 0.0100\n",
            "Epoch 48/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.2772 - mean_absolute_error: 0.2371 - mean_squared_error: 0.0891\n",
            "Epoch 00048: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 228us/sample - loss: 0.2772 - mean_absolute_error: 0.2371 - mean_squared_error: 0.0891 - val_loss: 0.1955 - val_mean_absolute_error: 0.0919 - val_mean_squared_error: 0.0093\n",
            "Epoch 49/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.2701 - mean_absolute_error: 0.2335 - mean_squared_error: 0.0859\n",
            "Epoch 00049: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 228us/sample - loss: 0.2702 - mean_absolute_error: 0.2337 - mean_squared_error: 0.0860 - val_loss: 0.1926 - val_mean_absolute_error: 0.0965 - val_mean_squared_error: 0.0102\n",
            "Epoch 50/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 0.2670 - mean_absolute_error: 0.2331 - mean_squared_error: 0.0863\n",
            "Epoch 00050: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 212us/sample - loss: 0.2669 - mean_absolute_error: 0.2330 - mean_squared_error: 0.0863 - val_loss: 0.1880 - val_mean_absolute_error: 0.0908 - val_mean_squared_error: 0.0091\n",
            "Epoch 51/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.2625 - mean_absolute_error: 0.2326 - mean_squared_error: 0.0854\n",
            "Epoch 00051: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 218us/sample - loss: 0.2627 - mean_absolute_error: 0.2329 - mean_squared_error: 0.0856 - val_loss: 0.1856 - val_mean_absolute_error: 0.0966 - val_mean_squared_error: 0.0102\n",
            "Epoch 52/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.2576 - mean_absolute_error: 0.2295 - mean_squared_error: 0.0839\n",
            "Epoch 00052: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 218us/sample - loss: 0.2576 - mean_absolute_error: 0.2297 - mean_squared_error: 0.0838 - val_loss: 0.1803 - val_mean_absolute_error: 0.0858 - val_mean_squared_error: 0.0082\n",
            "Epoch 53/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.2548 - mean_absolute_error: 0.2298 - mean_squared_error: 0.0843\n",
            "Epoch 00053: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 221us/sample - loss: 0.2548 - mean_absolute_error: 0.2298 - mean_squared_error: 0.0843 - val_loss: 0.1786 - val_mean_absolute_error: 0.0938 - val_mean_squared_error: 0.0097\n",
            "Epoch 54/100\n",
            "7904/8032 [============================>.] - ETA: 0s - loss: 0.2497 - mean_absolute_error: 0.2273 - mean_squared_error: 0.0823\n",
            "Epoch 00054: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 225us/sample - loss: 0.2496 - mean_absolute_error: 0.2271 - mean_squared_error: 0.0823 - val_loss: 0.1750 - val_mean_absolute_error: 0.0914 - val_mean_squared_error: 0.0093\n",
            "Epoch 55/100\n",
            "7904/8032 [============================>.] - ETA: 0s - loss: 0.2463 - mean_absolute_error: 0.2261 - mean_squared_error: 0.0820\n",
            "Epoch 00055: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 224us/sample - loss: 0.2464 - mean_absolute_error: 0.2264 - mean_squared_error: 0.0821 - val_loss: 0.1725 - val_mean_absolute_error: 0.0939 - val_mean_squared_error: 0.0097\n",
            "Epoch 56/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.2421 - mean_absolute_error: 0.2248 - mean_squared_error: 0.0808\n",
            "Epoch 00056: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 217us/sample - loss: 0.2421 - mean_absolute_error: 0.2248 - mean_squared_error: 0.0808 - val_loss: 0.1672 - val_mean_absolute_error: 0.0807 - val_mean_squared_error: 0.0073\n",
            "Epoch 57/100\n",
            "7744/8032 [===========================>..] - ETA: 0s - loss: 0.2386 - mean_absolute_error: 0.2247 - mean_squared_error: 0.0801\n",
            "Epoch 00057: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 214us/sample - loss: 0.2384 - mean_absolute_error: 0.2243 - mean_squared_error: 0.0799 - val_loss: 0.1658 - val_mean_absolute_error: 0.0885 - val_mean_squared_error: 0.0087\n",
            "Epoch 58/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 0.2349 - mean_absolute_error: 0.2234 - mean_squared_error: 0.0790\n",
            "Epoch 00058: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 213us/sample - loss: 0.2349 - mean_absolute_error: 0.2237 - mean_squared_error: 0.0791 - val_loss: 0.1622 - val_mean_absolute_error: 0.0831 - val_mean_squared_error: 0.0077\n",
            "Epoch 59/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.2331 - mean_absolute_error: 0.2233 - mean_squared_error: 0.0799\n",
            "Epoch 00059: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 215us/sample - loss: 0.2332 - mean_absolute_error: 0.2233 - mean_squared_error: 0.0800 - val_loss: 0.1589 - val_mean_absolute_error: 0.0786 - val_mean_squared_error: 0.0070\n",
            "Epoch 60/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.2291 - mean_absolute_error: 0.2215 - mean_squared_error: 0.0784\n",
            "Epoch 00060: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 217us/sample - loss: 0.2291 - mean_absolute_error: 0.2216 - mean_squared_error: 0.0785 - val_loss: 0.1587 - val_mean_absolute_error: 0.0920 - val_mean_squared_error: 0.0093\n",
            "Epoch 61/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 0.2273 - mean_absolute_error: 0.2227 - mean_squared_error: 0.0791\n",
            "Epoch 00061: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 218us/sample - loss: 0.2269 - mean_absolute_error: 0.2222 - mean_squared_error: 0.0787 - val_loss: 0.1536 - val_mean_absolute_error: 0.0764 - val_mean_squared_error: 0.0066\n",
            "Epoch 62/100\n",
            "7840/8032 [============================>.] - ETA: 0s - loss: 0.2219 - mean_absolute_error: 0.2191 - mean_squared_error: 0.0761\n",
            "Epoch 00062: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 212us/sample - loss: 0.2223 - mean_absolute_error: 0.2196 - mean_squared_error: 0.0765 - val_loss: 0.1540 - val_mean_absolute_error: 0.0926 - val_mean_squared_error: 0.0094\n",
            "Epoch 63/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.2184 - mean_absolute_error: 0.2174 - mean_squared_error: 0.0749\n",
            "Epoch 00063: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 212us/sample - loss: 0.2185 - mean_absolute_error: 0.2176 - mean_squared_error: 0.0750 - val_loss: 0.1500 - val_mean_absolute_error: 0.0829 - val_mean_squared_error: 0.0076\n",
            "Epoch 64/100\n",
            "7840/8032 [============================>.] - ETA: 0s - loss: 0.2177 - mean_absolute_error: 0.2179 - mean_squared_error: 0.0764\n",
            "Epoch 00064: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 224us/sample - loss: 0.2176 - mean_absolute_error: 0.2179 - mean_squared_error: 0.0764 - val_loss: 0.1479 - val_mean_absolute_error: 0.0835 - val_mean_squared_error: 0.0078\n",
            "Epoch 65/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.2151 - mean_absolute_error: 0.2192 - mean_squared_error: 0.0761\n",
            "Epoch 00065: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 216us/sample - loss: 0.2151 - mean_absolute_error: 0.2192 - mean_squared_error: 0.0761 - val_loss: 0.1468 - val_mean_absolute_error: 0.0903 - val_mean_squared_error: 0.0089\n",
            "Epoch 66/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.2123 - mean_absolute_error: 0.2172 - mean_squared_error: 0.0754\n",
            "Epoch 00066: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 218us/sample - loss: 0.2123 - mean_absolute_error: 0.2170 - mean_squared_error: 0.0754 - val_loss: 0.1416 - val_mean_absolute_error: 0.0708 - val_mean_squared_error: 0.0057\n",
            "Epoch 67/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.2073 - mean_absolute_error: 0.2124 - mean_squared_error: 0.0724\n",
            "Epoch 00067: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 212us/sample - loss: 0.2073 - mean_absolute_error: 0.2124 - mean_squared_error: 0.0724 - val_loss: 0.1410 - val_mean_absolute_error: 0.0802 - val_mean_squared_error: 0.0072\n",
            "Epoch 68/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.2059 - mean_absolute_error: 0.2128 - mean_squared_error: 0.0731\n",
            "Epoch 00068: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 215us/sample - loss: 0.2055 - mean_absolute_error: 0.2120 - mean_squared_error: 0.0728 - val_loss: 0.1401 - val_mean_absolute_error: 0.0871 - val_mean_squared_error: 0.0084\n",
            "Epoch 69/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.2028 - mean_absolute_error: 0.2116 - mean_squared_error: 0.0720\n",
            "Epoch 00069: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 216us/sample - loss: 0.2027 - mean_absolute_error: 0.2116 - mean_squared_error: 0.0720 - val_loss: 0.1366 - val_mean_absolute_error: 0.0780 - val_mean_squared_error: 0.0068\n",
            "Epoch 70/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.1995 - mean_absolute_error: 0.2097 - mean_squared_error: 0.0707\n",
            "Epoch 00070: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 216us/sample - loss: 0.1994 - mean_absolute_error: 0.2096 - mean_squared_error: 0.0707 - val_loss: 0.1364 - val_mean_absolute_error: 0.0883 - val_mean_squared_error: 0.0086\n",
            "Epoch 71/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.1981 - mean_absolute_error: 0.2115 - mean_squared_error: 0.0713\n",
            "Epoch 00071: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 215us/sample - loss: 0.1981 - mean_absolute_error: 0.2115 - mean_squared_error: 0.0713 - val_loss: 0.1338 - val_mean_absolute_error: 0.0843 - val_mean_squared_error: 0.0079\n",
            "Epoch 72/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.1953 - mean_absolute_error: 0.2101 - mean_squared_error: 0.0703\n",
            "Epoch 00072: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 216us/sample - loss: 0.1956 - mean_absolute_error: 0.2103 - mean_squared_error: 0.0706 - val_loss: 0.1314 - val_mean_absolute_error: 0.0809 - val_mean_squared_error: 0.0073\n",
            "Epoch 73/100\n",
            "7904/8032 [============================>.] - ETA: 0s - loss: 0.1943 - mean_absolute_error: 0.2110 - mean_squared_error: 0.0711\n",
            "Epoch 00073: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 212us/sample - loss: 0.1939 - mean_absolute_error: 0.2105 - mean_squared_error: 0.0708 - val_loss: 0.1300 - val_mean_absolute_error: 0.0835 - val_mean_squared_error: 0.0078\n",
            "Epoch 74/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.1911 - mean_absolute_error: 0.2079 - mean_squared_error: 0.0697\n",
            "Epoch 00074: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 221us/sample - loss: 0.1914 - mean_absolute_error: 0.2084 - mean_squared_error: 0.0701 - val_loss: 0.1274 - val_mean_absolute_error: 0.0788 - val_mean_squared_error: 0.0070\n",
            "Epoch 75/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.1884 - mean_absolute_error: 0.2085 - mean_squared_error: 0.0688\n",
            "Epoch 00075: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 223us/sample - loss: 0.1885 - mean_absolute_error: 0.2087 - mean_squared_error: 0.0690 - val_loss: 0.1261 - val_mean_absolute_error: 0.0812 - val_mean_squared_error: 0.0074\n",
            "Epoch 76/100\n",
            "7904/8032 [============================>.] - ETA: 0s - loss: 0.1862 - mean_absolute_error: 0.2062 - mean_squared_error: 0.0684\n",
            "Epoch 00076: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 223us/sample - loss: 0.1863 - mean_absolute_error: 0.2062 - mean_squared_error: 0.0684 - val_loss: 0.1226 - val_mean_absolute_error: 0.0695 - val_mean_squared_error: 0.0056\n",
            "Epoch 77/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.1838 - mean_absolute_error: 0.2060 - mean_squared_error: 0.0676\n",
            "Epoch 00077: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 212us/sample - loss: 0.1838 - mean_absolute_error: 0.2060 - mean_squared_error: 0.0676 - val_loss: 0.1233 - val_mean_absolute_error: 0.0850 - val_mean_squared_error: 0.0080\n",
            "Epoch 78/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.1827 - mean_absolute_error: 0.2068 - mean_squared_error: 0.0682\n",
            "Epoch 00078: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 214us/sample - loss: 0.1827 - mean_absolute_error: 0.2068 - mean_squared_error: 0.0681 - val_loss: 0.1204 - val_mean_absolute_error: 0.0768 - val_mean_squared_error: 0.0067\n",
            "Epoch 79/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 0.1807 - mean_absolute_error: 0.2059 - mean_squared_error: 0.0677\n",
            "Epoch 00079: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 215us/sample - loss: 0.1810 - mean_absolute_error: 0.2065 - mean_squared_error: 0.0680 - val_loss: 0.1185 - val_mean_absolute_error: 0.0748 - val_mean_squared_error: 0.0063\n",
            "Epoch 80/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 0.1771 - mean_absolute_error: 0.2018 - mean_squared_error: 0.0657\n",
            "Epoch 00080: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 213us/sample - loss: 0.1771 - mean_absolute_error: 0.2019 - mean_squared_error: 0.0658 - val_loss: 0.1194 - val_mean_absolute_error: 0.0899 - val_mean_squared_error: 0.0089\n",
            "Epoch 81/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.1740 - mean_absolute_error: 0.2006 - mean_squared_error: 0.0642\n",
            "Epoch 00081: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 214us/sample - loss: 0.1739 - mean_absolute_error: 0.2005 - mean_squared_error: 0.0641 - val_loss: 0.1162 - val_mean_absolute_error: 0.0799 - val_mean_squared_error: 0.0071\n",
            "Epoch 82/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.1744 - mean_absolute_error: 0.2037 - mean_squared_error: 0.0661\n",
            "Epoch 00082: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 213us/sample - loss: 0.1746 - mean_absolute_error: 0.2040 - mean_squared_error: 0.0663 - val_loss: 0.1136 - val_mean_absolute_error: 0.0728 - val_mean_squared_error: 0.0060\n",
            "Epoch 83/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.1703 - mean_absolute_error: 0.1992 - mean_squared_error: 0.0634\n",
            "Epoch 00083: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 222us/sample - loss: 0.1704 - mean_absolute_error: 0.1993 - mean_squared_error: 0.0636 - val_loss: 0.1117 - val_mean_absolute_error: 0.0698 - val_mean_squared_error: 0.0056\n",
            "Epoch 84/100\n",
            "7744/8032 [===========================>..] - ETA: 0s - loss: 0.1681 - mean_absolute_error: 0.1983 - mean_squared_error: 0.0628\n",
            "Epoch 00084: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 212us/sample - loss: 0.1680 - mean_absolute_error: 0.1983 - mean_squared_error: 0.0626 - val_loss: 0.1110 - val_mean_absolute_error: 0.0749 - val_mean_squared_error: 0.0064\n",
            "Epoch 85/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.1665 - mean_absolute_error: 0.1962 - mean_squared_error: 0.0625\n",
            "Epoch 00085: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 212us/sample - loss: 0.1665 - mean_absolute_error: 0.1963 - mean_squared_error: 0.0625 - val_loss: 0.1099 - val_mean_absolute_error: 0.0767 - val_mean_squared_error: 0.0067\n",
            "Epoch 86/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.1637 - mean_absolute_error: 0.1954 - mean_squared_error: 0.0612\n",
            "Epoch 00086: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 210us/sample - loss: 0.1637 - mean_absolute_error: 0.1952 - mean_squared_error: 0.0612 - val_loss: 0.1073 - val_mean_absolute_error: 0.0687 - val_mean_squared_error: 0.0055\n",
            "Epoch 87/100\n",
            "7808/8032 [============================>.] - ETA: 0s - loss: 0.1647 - mean_absolute_error: 0.1982 - mean_squared_error: 0.0636\n",
            "Epoch 00087: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 217us/sample - loss: 0.1646 - mean_absolute_error: 0.1982 - mean_squared_error: 0.0635 - val_loss: 0.1073 - val_mean_absolute_error: 0.0779 - val_mean_squared_error: 0.0068\n",
            "Epoch 88/100\n",
            "7840/8032 [============================>.] - ETA: 0s - loss: 0.1622 - mean_absolute_error: 0.1967 - mean_squared_error: 0.0624\n",
            "Epoch 00088: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 224us/sample - loss: 0.1622 - mean_absolute_error: 0.1966 - mean_squared_error: 0.0624 - val_loss: 0.1047 - val_mean_absolute_error: 0.0699 - val_mean_squared_error: 0.0056\n",
            "Epoch 89/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.1611 - mean_absolute_error: 0.1979 - mean_squared_error: 0.0627\n",
            "Epoch 00089: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 222us/sample - loss: 0.1609 - mean_absolute_error: 0.1975 - mean_squared_error: 0.0625 - val_loss: 0.1056 - val_mean_absolute_error: 0.0838 - val_mean_squared_error: 0.0078\n",
            "Epoch 90/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.1580 - mean_absolute_error: 0.1946 - mean_squared_error: 0.0608\n",
            "Epoch 00090: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 219us/sample - loss: 0.1579 - mean_absolute_error: 0.1947 - mean_squared_error: 0.0608 - val_loss: 0.1028 - val_mean_absolute_error: 0.0746 - val_mean_squared_error: 0.0063\n",
            "Epoch 91/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 0.1577 - mean_absolute_error: 0.1964 - mean_squared_error: 0.0619\n",
            "Epoch 00091: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 211us/sample - loss: 0.1579 - mean_absolute_error: 0.1968 - mean_squared_error: 0.0621 - val_loss: 0.1022 - val_mean_absolute_error: 0.0788 - val_mean_squared_error: 0.0070\n",
            "Epoch 92/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1543 - mean_absolute_error: 0.1927 - mean_squared_error: 0.0598\n",
            "Epoch 00092: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 216us/sample - loss: 0.1544 - mean_absolute_error: 0.1929 - mean_squared_error: 0.0599 - val_loss: 0.1020 - val_mean_absolute_error: 0.0854 - val_mean_squared_error: 0.0081\n",
            "Epoch 93/100\n",
            "7936/8032 [============================>.] - ETA: 0s - loss: 0.1525 - mean_absolute_error: 0.1917 - mean_squared_error: 0.0592\n",
            "Epoch 00093: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 216us/sample - loss: 0.1525 - mean_absolute_error: 0.1917 - mean_squared_error: 0.0592 - val_loss: 0.0990 - val_mean_absolute_error: 0.0741 - val_mean_squared_error: 0.0063\n",
            "Epoch 94/100\n",
            "7776/8032 [============================>.] - ETA: 0s - loss: 0.1508 - mean_absolute_error: 0.1905 - mean_squared_error: 0.0586\n",
            "Epoch 00094: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 213us/sample - loss: 0.1504 - mean_absolute_error: 0.1900 - mean_squared_error: 0.0583 - val_loss: 0.0998 - val_mean_absolute_error: 0.0866 - val_mean_squared_error: 0.0083\n",
            "Epoch 95/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.1493 - mean_absolute_error: 0.1907 - mean_squared_error: 0.0584\n",
            "Epoch 00095: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 222us/sample - loss: 0.1493 - mean_absolute_error: 0.1906 - mean_squared_error: 0.0584 - val_loss: 0.0954 - val_mean_absolute_error: 0.0660 - val_mean_squared_error: 0.0051\n",
            "Epoch 96/100\n",
            "7840/8032 [============================>.] - ETA: 0s - loss: 0.1485 - mean_absolute_error: 0.1910 - mean_squared_error: 0.0588\n",
            "Epoch 00096: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 226us/sample - loss: 0.1487 - mean_absolute_error: 0.1914 - mean_squared_error: 0.0590 - val_loss: 0.0959 - val_mean_absolute_error: 0.0769 - val_mean_squared_error: 0.0067\n",
            "Epoch 97/100\n",
            "7840/8032 [============================>.] - ETA: 0s - loss: 0.1459 - mean_absolute_error: 0.1886 - mean_squared_error: 0.0573\n",
            "Epoch 00097: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 224us/sample - loss: 0.1459 - mean_absolute_error: 0.1884 - mean_squared_error: 0.0573 - val_loss: 0.0950 - val_mean_absolute_error: 0.0785 - val_mean_squared_error: 0.0070\n",
            "Epoch 98/100\n",
            "7968/8032 [============================>.] - ETA: 0s - loss: 0.1449 - mean_absolute_error: 0.1886 - mean_squared_error: 0.0574\n",
            "Epoch 00098: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 228us/sample - loss: 0.1448 - mean_absolute_error: 0.1885 - mean_squared_error: 0.0573 - val_loss: 0.0933 - val_mean_absolute_error: 0.0750 - val_mean_squared_error: 0.0064\n",
            "Epoch 99/100\n",
            "8000/8032 [============================>.] - ETA: 0s - loss: 0.1428 - mean_absolute_error: 0.1871 - mean_squared_error: 0.0565\n",
            "Epoch 00099: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 227us/sample - loss: 0.1428 - mean_absolute_error: 0.1871 - mean_squared_error: 0.0564 - val_loss: 0.0924 - val_mean_absolute_error: 0.0762 - val_mean_squared_error: 0.0066\n",
            "Epoch 100/100\n",
            "7872/8032 [============================>.] - ETA: 0s - loss: 0.1409 - mean_absolute_error: 0.1857 - mean_squared_error: 0.0556\n",
            "Epoch 00100: val_loss did not improve from 0.00143\n",
            "8032/8032 [==============================] - 2s 218us/sample - loss: 0.1407 - mean_absolute_error: 0.1853 - mean_squared_error: 0.0554 - val_loss: 0.0915 - val_mean_absolute_error: 0.0773 - val_mean_squared_error: 0.0068\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e_OdY7oIGsTj",
        "colab_type": "code",
        "outputId": "40b71c33-071b-4fc6-fc97-b674fd8d42ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "histories"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.callbacks.History at 0x7f4841ee6278>,\n",
              " <tensorflow.python.keras.callbacks.History at 0x7f4838e37470>,\n",
              " <tensorflow.python.keras.callbacks.History at 0x7f4838317860>,\n",
              " <tensorflow.python.keras.callbacks.History at 0x7f4837e21278>,\n",
              " <tensorflow.python.keras.callbacks.History at 0x7f483786f978>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "zP9DnUcdHqDD",
        "colab_type": "code",
        "outputId": "195494dc-9a22-46f9-f1cf-1af246c4c735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "histories[0].history.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'mean_absolute_error', 'mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "DT6uuSPSZ_hb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Plot"
      ]
    },
    {
      "metadata": {
        "id": "kwDly9jkHNpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  \n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Abs Error [MPG]')\n",
        "  plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
        "           label='Train Error')\n",
        "  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
        "           label = 'Val Error')\n",
        "  plt.ylim([0,5])\n",
        "  plt.legend()\n",
        "  \n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Square Error [$MPG^2$]')\n",
        "  plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
        "           label='Train Error')\n",
        "  plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
        "           label = 'Val Error')\n",
        "  plt.ylim([0,20])\n",
        "  plt.legend()\n",
        "  \n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(hist['epoch'], hist['loss'],\n",
        "           label='Train loss')\n",
        "  plt.plot(hist['epoch'], hist['val_loss'],\n",
        "           label = 'Val loss')\n",
        "  plt.ylim([0,20])\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rv3KesovaFGi",
        "colab_type": "code",
        "outputId": "59b6b344-dcfc-470b-bcb2-ed0688d9132f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1061
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFcCAYAAADyAHbiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf4AAAFcCAYAAAApu9zAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfhERkRCi4BcREQkh/z+oJjTPd7xEOwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFcCAYAAADRWyc3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERERi/j/AcaPdL53Q96rAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "TONMLUpMuB9I",
        "colab_type": "code",
        "outputId": "04e4d274-b191-4648-c24e-e97bb8844406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#models[2].evaluate(X_test, Y_test, batch_size=BATCH_SIZE)\n",
        "a = X_val[2014:]\n",
        "a.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 32, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "metadata": {
        "id": "vp9SZ7vMuu58",
        "colab_type": "code",
        "outputId": "318196de-715a-4a58-cf9a-fb14481f4a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "b = Y_val[2014:]\n",
        "b.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "metadata": {
        "id": "E1CmQABluz9d",
        "colab_type": "code",
        "outputId": "4af3f0fd-87d2-4b23-86af-c6744f466e55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#t1 = np.arange(2*7*4).reshape(2,7,4)\n",
        "#t2 = (np.arange(2*7*4)+10000).reshape(2,7,4)\n",
        "c = X_test\n",
        "r = np.concatenate((a, c), axis=0)\n",
        "r.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 32, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "metadata": {
        "id": "kaxxy_Egv6M0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = np.concatenate((b, Y_test), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xNivcI-MwBK5",
        "colab_type": "code",
        "outputId": "4376eac9-047a-437c-d557-9ec8bcf3cd4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "t.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "metadata": {
        "id": "GDLr04XfwCIj",
        "colab_type": "code",
        "outputId": "986657ea-a8cb-42f0-eb14-d4040145b88a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#print(\"r0:\"+str(r[0]))\n",
        "print(\"t0:\"+str(t[0]))\n",
        "pred = models[2].predict(r)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t0:1.1438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xcmc1Q51wd4Z",
        "colab_type": "code",
        "outputId": "6caf1275-eccb-4eb7-b2cd-d9e2e59d00e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "for m in models:\n",
        "  pred = m.predict(r)\n",
        "  acc = 0\n",
        "  for i in range(len(pred)):\n",
        "    acc += (pred[i]/t[i])*100\n",
        "\n",
        "  acc /= len(pred)\n",
        "  print(acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[54.30987]\n",
            "[64.725975]\n",
            "[95.712456]\n",
            "[87.4359]\n",
            "[94.76881]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Osx8uHR8xbzO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_4 = models[-1].predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3YRCl0v-xfVZ",
        "colab_type": "code",
        "outputId": "f1653ce3-1e97-4e0d-8f7e-36dca6853be2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pred_4[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.07751], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "metadata": {
        "id": "5b92n3ciyANs",
        "colab_type": "code",
        "outputId": "ba220654-c388-4c17-ed72-e00246bc87ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "Y_test[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1295"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "metadata": {
        "id": "oPNIeRWMyT3b",
        "colab_type": "code",
        "outputId": "394409cc-2fb4-4a6c-c6c9-72fefb3eaa24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-27b57dd420a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'val_loss'"
          ]
        }
      ]
    }
  ]
}