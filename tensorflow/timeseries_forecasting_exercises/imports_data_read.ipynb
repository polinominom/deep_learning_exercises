{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"imports_data_read.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"fQ3Ueq1vBFgZ","colab_type":"text"},"cell_type":"markdown","source":["# Explanation\n","\n","**import**\n","\n","\n","In this exercise, we try to import all the important modules and respective functions in a way that this import style will be used in every exercise.\n","\n","**data**\n","\n","Data read and data load ways also tested here and found one way to use it every exercise.\n","\n","\n"]},{"metadata":{"id":"u66p6NvWwIo1","colab_type":"text"},"cell_type":"markdown","source":["# Imports"]},{"metadata":{"id":"p86yXuBstggu","colab_type":"code","colab":{}},"cell_type":"code","source":["from __future__ import print_function, absolute_import, division\n","\n","# general imports for deep learning\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn import preprocessing\n","\n","# data read\n","import pandas as pd\n","\n","# plot\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.utils import plot_model\n","\n","# json and pretty print\n","import json\n","import pprint\n","\n","# to persist the numpy arrays data\n","import h5py\n","\n","# handle logging\n","tf.logging.set_verbosity(tf.logging.INFO)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6lbQb9MJwKDC","colab_type":"text"},"cell_type":"markdown","source":["# Mount Google Drive"]},{"metadata":{"id":"BMRzG1H3tsXt","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Jz8Mmkl8A93X","colab_type":"code","colab":{}},"cell_type":"code","source":["# check if correct place\n","!ls '/content/gdrive/My Drive/deep_learning/_data/forex/minutely/'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nMr1v4YDwSeu","colab_type":"text"},"cell_type":"markdown","source":["# Data Read and Concatenating"]},{"metadata":{"id":"iznHT_yUwUCF","colab_type":"code","colab":{}},"cell_type":"code","source":["# Any data folder name and file names can be given.\n","# initialize file names\n","data_folder = \"/content/gdrive/My Drive/deep_learning/_data/forex/minutely/\"\n","data_filenames = []\n","data_filenames.append(\"EURUSD_M1_2013.csv\")\n","data_filenames.append(\"EURUSD_M1_2014.csv\")\n","data_filenames.append(\"EURUSD_M1_2015.csv\")\n","data_filenames.append(\"EURUSD_M1_2016.csv\")\n","data_filenames.append(\"EURUSD_M1_2017.csv\")\n","data_filenames.append(\"EURUSD_M1_2018.csv\")\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9lNK9__UwiGg","colab_type":"code","colab":{}},"cell_type":"code","source":["# get train data that will be both validation and train data in training mode\n","data_1 = pd.read_csv(data_folder+data_filenames[0], header=None)\n","data_2 = pd.read_csv(data_folder+data_filenames[1], header=None)\n","data_3 = pd.read_csv(data_folder+data_filenames[2], header=None)\n","data_4 = pd.read_csv(data_folder+data_filenames[3], header=None)\n","data_5 = pd.read_csv(data_folder+data_filenames[4], header=None)\n","data_6 = pd.read_csv(data_folder+data_filenames[5], header=None)\n","\n","# Get all data as list\n","data_list = [data_1, data_2, data_3, data_4, data_5, data_6]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cvCMJcEAxEBA","colab_type":"code","colab":{}},"cell_type":"code","source":["# Tries to concatenate a list of arrays into one array\n","def get_concatenated_dataset(d_list):\n","  result_data = d_list[0]\n","  for d in d_list[1:]:\n","    result_data = np.concatenate((result_data, d), axis=None)\n","    \n","  return result_data\n","# Tries to check if the concatenated list is correct.\n","def concatenate_length_check(d_list, concatenated):\n","  print(\"----------- length check -----------\")\n","  total_length = 0\n","  for d in d_list:  \n","    total_length += len(d)\n","    print(\"length: \" +str(len(d)))\n","\n","  print(\"concatenated length \"+str(len(concatenated)))\n","  if(len(concatenated) == total_length):\n","    print(\"concatenated length -----------> CORRECT\")\n","  else:\n","    print(\"concatenated length -----------> WRONG\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lhxMaKAgxcnR","colab_type":"text"},"cell_type":"markdown","source":["These functions above are especially useful in a multivariate forecast. Example  code is below.\n","\n","Multivariate forecast = Close data is not only feature data to be looked at, open, high, low will also be taken into consideration by the deep learning model. However, it can be extended such as adding new feature data for example technical indicators EMA etc."]},{"metadata":{"id":"ApZfB14qxXda","colab_type":"code","colab":{}},"cell_type":"code","source":["# READING DATA FOR MULTIVARIATE FORECAST\n","\n","# Get OPEN, HIGH, LOW, CLOSE columns in all data\n","open_data_list=[]\n","high_data_list=[]\n","low_data_list=[]\n","close_data_list=[]\n","\n","for d in data_list:\n","  open_data_list.append(d[2].as_matrix())\n","  high_data_list.append(d[3].as_matrix())\n","  low_data_list.append(d[4].as_matrix())\n","  close_data_list.append(d[5].as_matrix())\n","  \n","# And CONCATENATE all of them\n","all_open_data = get_concatenated_dataset(open_data_list)\n","all_high_data = get_concatenated_dataset(high_data_list)\n","all_low_data = get_concatenated_dataset(low_data_list)\n","all_close_data = get_concatenated_dataset(close_data_list)\n","  \n","# CHECK IF CONCATENATION IS SUCCESSUL.\n","concatenate_length_check(open_data_list, all_open_data)\n","concatenate_length_check(high_data_list, all_high_data)\n","concatenate_length_check(low_data_list, all_low_data)\n","concatenate_length_check(close_data_list, all_close_data)"],"execution_count":0,"outputs":[]}]}